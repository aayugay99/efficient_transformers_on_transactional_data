# Efficient transformers on transactional data

One of the drawbacks of transformer models is quadratic complexity $\mathcal{O}(N^2)$ of attention mechanism with respect to input sequence length $N$. Various modifications were proposed by researchers to overcome this problem. The goal of this project is to train different types of efficient transformers on transactional data and compare their performance in terms of accuracy and training time with standard transformer.
