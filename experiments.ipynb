{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import KBinsDiscretizer\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn \n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "\n",
    "from dataset import TransactionDataset, transaction_collate_fn\n",
    "\n",
    "from torchmetrics.classification import MulticlassF1Score, Accuracy\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from reformer_pytorch import Reformer\n",
    "\n",
    "import time\n",
    "\n",
    "import wandb\n",
    "from functools import partial\n",
    "from typing import Union\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>PERIOD</th>\n",
       "      <th>client_id</th>\n",
       "      <th>small_group</th>\n",
       "      <th>channel_type</th>\n",
       "      <th>currency</th>\n",
       "      <th>TRDATETIME</th>\n",
       "      <th>amount_rur</th>\n",
       "      <th>trx_category</th>\n",
       "      <th>target_flag</th>\n",
       "      <th>target_sum</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>01/10/2017</td>\n",
       "      <td>0</td>\n",
       "      <td>5200</td>\n",
       "      <td>NaN</td>\n",
       "      <td>810</td>\n",
       "      <td>2017-10-21 00:00:00</td>\n",
       "      <td>5023.0</td>\n",
       "      <td>POS</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>01/10/2017</td>\n",
       "      <td>0</td>\n",
       "      <td>6011</td>\n",
       "      <td>NaN</td>\n",
       "      <td>810</td>\n",
       "      <td>2017-10-12 12:24:07</td>\n",
       "      <td>20000.0</td>\n",
       "      <td>DEPOSIT</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>01/12/2017</td>\n",
       "      <td>0</td>\n",
       "      <td>5921</td>\n",
       "      <td>NaN</td>\n",
       "      <td>810</td>\n",
       "      <td>2017-12-05 00:00:00</td>\n",
       "      <td>767.0</td>\n",
       "      <td>POS</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>01/10/2017</td>\n",
       "      <td>0</td>\n",
       "      <td>5411</td>\n",
       "      <td>NaN</td>\n",
       "      <td>810</td>\n",
       "      <td>2017-10-21 00:00:00</td>\n",
       "      <td>2031.0</td>\n",
       "      <td>POS</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>01/10/2017</td>\n",
       "      <td>0</td>\n",
       "      <td>6012</td>\n",
       "      <td>NaN</td>\n",
       "      <td>810</td>\n",
       "      <td>2017-10-24 13:14:24</td>\n",
       "      <td>36562.0</td>\n",
       "      <td>C2C_OUT</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       PERIOD  client_id  small_group channel_type  currency  \\\n",
       "0  01/10/2017          0         5200          NaN       810   \n",
       "1  01/10/2017          0         6011          NaN       810   \n",
       "2  01/12/2017          0         5921          NaN       810   \n",
       "3  01/10/2017          0         5411          NaN       810   \n",
       "4  01/10/2017          0         6012          NaN       810   \n",
       "\n",
       "           TRDATETIME  amount_rur trx_category  target_flag  target_sum  \n",
       "0 2017-10-21 00:00:00      5023.0          POS            0         0.0  \n",
       "1 2017-10-12 12:24:07     20000.0      DEPOSIT            0         0.0  \n",
       "2 2017-12-05 00:00:00       767.0          POS            0         0.0  \n",
       "3 2017-10-21 00:00:00      2031.0          POS            0         0.0  \n",
       "4 2017-10-24 13:14:24     36562.0      C2C_OUT            0         0.0  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('data/rosbank/train.csv')\n",
    "df['TRDATETIME'] = pd.to_datetime(df['TRDATETIME'], format='%d%b%y:%H:%M:%S')\n",
    "df = df.rename(columns={'cl_id':'client_id', 'MCC':'small_group', 'amount':'amount_rur'})\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "mcc_to_id = {mcc: i+1 for i, mcc in enumerate(df['small_group'].unique())}\n",
    "\n",
    "df['amount_rur_bin'] = 1 + KBinsDiscretizer(10, encode='ordinal', subsample=None).fit_transform(df[['amount_rur']]).astype('int')\n",
    "df['small_group'] = df['small_group'].map(mcc_to_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "628 sequences were filtered\n",
      "77 sequences were filtered\n",
      "70 sequences were filtered\n"
     ]
    }
   ],
   "source": [
    "clients_train, clients_val_test = train_test_split(df[\"client_id\"].unique(), test_size=0.2, random_state=42)\n",
    "clients_val, clients_test = train_test_split(clients_val_test, test_size=0.5, random_state=42)\n",
    "\n",
    "train_ds = TransactionDataset(\n",
    "    df[lambda x: x[\"client_id\"].isin(clients_train)], \n",
    "    id_col=\"client_id\", \n",
    "    dt_col=\"TRDATETIME\", \n",
    "    cat_cols=[\"small_group\", \"amount_rur_bin\"],\n",
    "    min_length=20,\n",
    "    max_length=100\n",
    ")\n",
    "\n",
    "val_ds = TransactionDataset(\n",
    "    df[lambda x: x[\"client_id\"].isin(clients_val)], \n",
    "    id_col=\"client_id\", \n",
    "    dt_col=\"TRDATETIME\", \n",
    "    cat_cols=[\"small_group\", \"amount_rur_bin\"],\n",
    "    min_length=20,\n",
    "    max_length=100\n",
    ")\n",
    "\n",
    "test_ds = TransactionDataset(\n",
    "    df[lambda x: x[\"client_id\"].isin(clients_test)], \n",
    "    id_col=\"client_id\", \n",
    "    dt_col=\"TRDATETIME\", \n",
    "    cat_cols=[\"small_group\", \"amount_rur_bin\"],\n",
    "    min_length=20,\n",
    "    max_length=100\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mooodnakov\u001b[0m (\u001b[33maayugay99\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.15.3 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.14.2"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/mnt/Vol0/home/alex/ml_pr/efficient_transformers_on_transactional_data/wandb/run-20230523_130245-r6gpeaph</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/aayugay99/deep-learning-project/runs/r6gpeaph' target=\"_blank\">reformer_len_100_run_1</a></strong> to <a href='https://wandb.ai/aayugay99/deep-learning-project' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/aayugay99/deep-learning-project' target=\"_blank\">https://wandb.ai/aayugay99/deep-learning-project</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/aayugay99/deep-learning-project/runs/r6gpeaph' target=\"_blank\">https://wandb.ai/aayugay99/deep-learning-project/runs/r6gpeaph</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<button onClick=\"this.nextSibling.style.display='block';this.style.display='none';\">Display W&B run</button><iframe src='https://wandb.ai/aayugay99/deep-learning-project/runs/r6gpeaph?jupyter=true' style='border:none;width:100%;height:420px;display:none;'></iframe>"
      ],
      "text/plain": [
       "<wandb.sdk.wandb_run.Run at 0x7fe4410c2b90>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "config = {\n",
    "    \"experiment_name\": \"reformer_len_100_run_1\",\n",
    "    \"dataset\": \"rosbank\",\n",
    "    \"min_length\": 20,\n",
    "    \"max_length\": 100,\n",
    "    \"batch_size\": 32,\n",
    "    \"type\": \"transformer\",\n",
    "    \"transformer_params\": {\n",
    "        \"feature_embeddings\": {\"small_group\": (345, 64), \"amount_rur_bin\": (11, 64)}, \n",
    "        \"linear_proj\": 64,\n",
    "        \"n_head\": 8, \n",
    "        \"dim_feedforward\": 128, \n",
    "        \"dropout\": 0.1, \n",
    "        \"num_layers\": 6, \n",
    "        \"head_hidden\": 128,\n",
    "    },\n",
    "    \"lr\": 1e-3,\n",
    "    \"n_epochs\": 50,\n",
    "    \"warmup\": 10,\n",
    "    \"device\": \"cuda\",\n",
    "    \"save_path\": \"./\",\n",
    "    \"tags\":[\"reformder\"]\n",
    "}\n",
    "wandb.init(\n",
    "        project=\"deep-learning-project\",\n",
    "        entity=\"aayugay99\",\n",
    "        name=config[\"experiment_name\"], \n",
    "        tags=config[\"tags\"],\n",
    "        config=config\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class TransactionEncoder(nn.Module):\n",
    "    def __init__(self, feature_embeddings: dict[str, tuple[int, int]], linear_proj: int=None):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.feature_embeddings = feature_embeddings\n",
    "        self.embeddings = nn.ModuleDict({key: nn.Embedding(vocab, dim) for key, (vocab, dim) in feature_embeddings.items()})\n",
    "        \n",
    "        if linear_proj is not None:\n",
    "            self.embedding_dim = linear_proj\n",
    "            self.linear_proj = nn.Linear(sum([dim for key, (vocab, dim) in feature_embeddings.items()]), linear_proj)\n",
    "        else:\n",
    "            self.embedding_dim = sum([dim for key, (vocab, dim) in feature_embeddings.items()])\n",
    "            self.linear_proj = nn.Identity()\n",
    "\n",
    "    def forward(self, x: torch.Tensor, device: str=\"cpu\") -> torch.Tensor:\n",
    "        embeddings = [self.embeddings[key](x[key].to(device)) for key in self.feature_embeddings]\n",
    "        proj = self.linear_proj(torch.cat(embeddings, dim=2))\n",
    "        return proj\n",
    "\n",
    "\n",
    "class Head(nn.Module):\n",
    "    def __init__(self, embedding_dim: int, hidden_dim: int, vocab_size: int):\n",
    "        super().__init__()\n",
    "\n",
    "        self.head = nn.Sequential(\n",
    "            nn.Linear(embedding_dim, hidden_dim),\n",
    "            nn.GELU(),\n",
    "            nn.Linear(hidden_dim, vocab_size),\n",
    "        )\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        return self.head(x)\n",
    "\n",
    "\n",
    "class PositionalEncoding(nn.Module):\n",
    "    def __init__(self, embedding_dim: int, dropout: float=0.1, max_len: int=1000):\n",
    "        super().__init__()\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "        position = torch.arange(max_len).unsqueeze(1)\n",
    "        div_term = torch.exp(torch.arange(0, embedding_dim, 2) * (-math.log(10000.0) / embedding_dim))\n",
    "        pe = torch.zeros(1, max_len, embedding_dim)\n",
    "        pe[0, :, 0::2] = torch.sin(position * div_term)\n",
    "        pe[0, :, 1::2] = torch.cos(position * div_term)\n",
    "        self.register_buffer('pe', pe)\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Arguments:\n",
    "            x: Tensor, shape ``[batch_size, seq_len, embedding_dim]``\n",
    "        \"\"\"\n",
    "        x = x + self.pe[:, :x.size(1), :]\n",
    "        return self.dropout(x)\n",
    "\n",
    "class ReformerModel(nn.Module):\n",
    "    def __init__(\n",
    "            self, \n",
    "            feature_embeddings: dict[str, tuple[int, int]], \n",
    "            linear_proj: int=None,\n",
    "            n_head: int=8, \n",
    "            dim_feedforward: int=128, \n",
    "            dropout: float=0.1, \n",
    "            num_layers: int=6, \n",
    "            bucket_size: int=25,\n",
    "            head_hidden: int=128,\n",
    "            max_len: int=1000,\n",
    "        ):\n",
    "        super().__init__()\n",
    "\n",
    "        self.transaction_encoder = TransactionEncoder(feature_embeddings, linear_proj=linear_proj)\n",
    "        self.embedding_dim = self.transaction_encoder.embedding_dim\n",
    "        self.cat_cols = list(feature_embeddings.keys())\n",
    "        self.num_classes_dict = {key: num_classes for key, (num_classes, _) in feature_embeddings.items()}\n",
    "        \n",
    "        self.pos_emb = PositionalEncoding(self.embedding_dim, dropout, max_len)\n",
    "\n",
    "        \n",
    "        self.to_model_dim = nn.Identity() if self.embedding_dim == dim_feedforward else nn.Linear(self.embedding_dim, dim_feedforward)\n",
    "        \n",
    "        self.transformer_encoder = Reformer(\n",
    "            dim = dim_feedforward, \n",
    "            depth = num_layers,\n",
    "            heads = n_head, \n",
    "            bucket_size = bucket_size, \n",
    "            ff_dropout = dropout,\n",
    "        )\n",
    "        self.norm = nn.LayerNorm(dim_feedforward)\n",
    "        self.out = nn.Linear(dim_feedforward, self.embedding_dim) if self.embedding_dim != dim_feedforward else Identity()\n",
    "        self.heads = nn.ModuleDict({\n",
    "            key: Head(\n",
    "                self.embedding_dim, \n",
    "                head_hidden, \n",
    "                num_classes\n",
    "            ) for key, num_classes in self.num_classes_dict.items()\n",
    "        })\n",
    "\n",
    "    def forward(self, x: torch.Tensor, device: Union[str, torch.device]=\"cpu\") -> torch.Tensor:\n",
    "        N, S = x[self.cat_cols[0]].shape\n",
    "        embeddings = self.transaction_encoder(x, device=device)\n",
    "        print(embeddings.shape)\n",
    "        embeddings = self.pos_emb(embeddings)\n",
    "        print(embeddings.shape)\n",
    "        \n",
    "        attn_mask = self.generate_square_subsequent_mask(S).to(device)\n",
    "        padding_mask = self.generate_padding_mask(x[self.cat_cols[0]]).to(device)\n",
    "        embeddings = self.to_model_dim(embeddings)\n",
    "        print(embeddings.shape)\n",
    "        embeddings = self.transformer_encoder(embeddings, mask=attn_mask, casual=True, src_key_padding_mask=padding_mask)\n",
    "        print(embeddings.shape)\n",
    "        embeddings = self.norm(embeddings)\n",
    "        print(embeddings.shape)\n",
    "        embeddings = self.out(embeddings)\n",
    "        print(embeddings.shape)\n",
    "        logits = {key: self.heads[key](embeddings) for key in self.cat_cols}\n",
    "        \n",
    "        return logits\n",
    "\n",
    "    @staticmethod\n",
    "    def generate_square_subsequent_mask(sz: int) -> torch.Tensor:\n",
    "        return torch.triu(torch.full((sz, sz), True), diagonal=1).bool()\n",
    "    \n",
    "    @staticmethod\n",
    "    def generate_padding_mask(x: torch.Tensor) -> torch.Tensor:\n",
    "        return torch.where(x == 0, True, False).bool()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "def train_epoch(model, optimizer, dataloader, warmup=10, device=\"cuda\"):\n",
    "    model.train()\n",
    "    model.to(device)\n",
    "\n",
    "    metrics = {\n",
    "        key: {\n",
    "            \"f1_score\": MulticlassF1Score(\n",
    "                num_classes=num_classes, \n",
    "                average=\"weighted\", \n",
    "                ignore_index=0\n",
    "            ), \n",
    "            \"accuracy\": Accuracy(\n",
    "                task=\"multiclass\", \n",
    "                num_classes=num_classes, \n",
    "                ignore_index=0\n",
    "            )\n",
    "        } for key, num_classes in model.num_classes_dict.items()\n",
    "    }\n",
    "\n",
    "    loss_epoch = 0\n",
    "    count = 0 \n",
    "    for batch_dict in dataloader:\n",
    "        logits_dict = model(batch_dict, device=device)\n",
    "\n",
    "        loss = 0\n",
    "        for key, logits in logits_dict.items():\n",
    "            y = batch_dict[key][:, warmup + 1:].to(device)\n",
    "            logits_pred = logits[:, warmup: -1].permute(0, 2, 1)    # B x C x T\n",
    "\n",
    "            loss += nn.functional.cross_entropy(logits_pred, y, ignore_index=0)\n",
    "            \n",
    "            y_pred = logits_pred.argmax(dim=1).to(\"cpu\")\n",
    "            metrics[key][\"f1_score\"].update(y_pred, y.to(\"cpu\"))\n",
    "            metrics[key][\"accuracy\"].update(y_pred, y.to(\"cpu\"))\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        cur_count =  torch.sum((y != 0).float()).item()\n",
    "        loss_epoch += loss.item() * cur_count\n",
    "        count += cur_count\n",
    "\n",
    "    return loss_epoch / count, {feature: {m: v.compute().item() for m, v in results.items()} for feature, results in metrics.items()}\n",
    "\n",
    "\n",
    "def eval_epoch(model, dataloader, warmup=10, device=\"cuda\"):\n",
    "    model.eval()\n",
    "    model.to(device)\n",
    "\n",
    "    metrics = {\n",
    "        key: {\n",
    "            \"f1_score\": MulticlassF1Score(\n",
    "                num_classes=num_classes, \n",
    "                average=\"weighted\", \n",
    "                ignore_index=0\n",
    "            ), \n",
    "            \"accuracy\": Accuracy(\n",
    "                task=\"multiclass\", \n",
    "                num_classes=num_classes, \n",
    "                ignore_index=0\n",
    "            )\n",
    "        } for key, num_classes in model.num_classes_dict.items()\n",
    "    }\n",
    "\n",
    "    with torch.no_grad():\n",
    "        loss_epoch = 0\n",
    "        count = 0 \n",
    "        for batch_dict in dataloader:\n",
    "            logits_dict = model(batch_dict, device=device)\n",
    "\n",
    "            loss = 0\n",
    "            for key, logits in logits_dict.items():\n",
    "                y = batch_dict[key][:, warmup + 1:].to(device)\n",
    "                logits_pred = logits[:, warmup: -1].permute(0, 2, 1)\n",
    "        \n",
    "                loss += nn.functional.cross_entropy(logits_pred, y, ignore_index=0)\n",
    "\n",
    "                y_pred = logits_pred.argmax(dim=1).to(\"cpu\")\n",
    "                metrics[key][\"f1_score\"].update(y_pred, y.to(\"cpu\"))\n",
    "                metrics[key][\"accuracy\"].update(y_pred, y.to(\"cpu\"))\n",
    "\n",
    "            cur_count = torch.sum((y != 0).float()).item()\n",
    "            loss_epoch += loss.item() * cur_count\n",
    "            count += cur_count    \n",
    "\n",
    "    return loss_epoch / count, {feature: {m: v.compute().item() for m, v in results.items()} for feature, results in metrics.items()}\n",
    "\n",
    "\n",
    "def train_model(model, optimizer, dataloaders, n_epochs, warmup=10, device=\"cuda\", save_path=\"./\"):\n",
    "    if not os.path.exists(save_path):\n",
    "        os.mkdir(save_path)\n",
    "\n",
    "    best_loss = float(\"inf\")\n",
    "    for epoch in range(n_epochs):\n",
    "        train_start = time.perf_counter()\n",
    "        train_loss, train_metrics = train_epoch(model, optimizer, dataloaders[\"train\"], warmup, device)\n",
    "        train_end = time.perf_counter()\n",
    "        val_loss, val_metrics = eval_epoch(model, dataloaders[\"val\"], warmup, device)\n",
    "        val_end = time.perf_counter()\n",
    "\n",
    "        wandb.log({\n",
    "            \"Epoch\": epoch+1,\n",
    "            \"Train time\": train_end - train_start,\n",
    "            \"Train loss\": train_loss,\n",
    "            \"Train metrics\": train_metrics,\n",
    "            \"Val time\": val_end - train_end,\n",
    "            \"Val metrics\": val_metrics,\n",
    "            \"Val loss\": val_loss\n",
    "        })\n",
    "\n",
    "        if val_loss < best_loss:\n",
    "             best_loss = val_loss\n",
    "             torch.save(model, os.path.join(save_path, \"best_model.pt\"))\n",
    "\n",
    "    model = torch.load(os.path.join(save_path, \"best_model.pt\"))\n",
    "\n",
    "    test_start = time.perf_counter()\n",
    "    test_loss, test_metrics = eval_epoch(model, dataloaders[\"test\"], warmup, device)\n",
    "    test_end = time.perf_counter()\n",
    "\n",
    "    wandb.summary[\"Test time\"] = test_end - test_start\n",
    "    wandb.summary[\"Test metrics\"] = test_metrics\n",
    "    wandb.summary[\"Test loss\"] = test_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = DataLoader(train_ds, batch_size=32, shuffle=True, collate_fn=partial(transaction_collate_fn,100))\n",
    "val_loader = DataLoader(val_ds, batch_size=32, shuffle=False, collate_fn=partial(transaction_collate_fn,100))\n",
    "test_loader = DataLoader(test_ds, batch_size=32, shuffle=False, collate_fn=partial(transaction_collate_fn,100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "transformer = ReformerModel(\n",
    "    feature_embeddings={\"small_group\": (345, 64), \"amount_rur_bin\": (11, 64)}, \n",
    "    linear_proj=64,\n",
    "    n_head=8, \n",
    "    dim_feedforward=1024, \n",
    "    dropout=0.1, \n",
    "    num_layers=6, \n",
    "    head_hidden=128\n",
    ").to('cuda')\n",
    "\n",
    "optimizer = torch.optim.Adam(transformer.parameters(), lr=1e-3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([32, 100, 64])\n",
      "torch.Size([32, 100, 64])\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Given normalized_shape=[1024], expected input with shape [*, 1024], but got input of size[32, 100, 64]",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[71], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m transformer(\u001b[39mnext\u001b[39m(\u001b[39miter\u001b[39m(train_loader)),device\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mcuda\u001b[39m\u001b[39m'\u001b[39m)\n",
      "File \u001b[0;32m~/personal/anaconda3/envs/main4/lib/python3.11/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[1;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "Cell \u001b[0;32mIn[69], line 106\u001b[0m, in \u001b[0;36mReformerModel.forward\u001b[0;34m(self, x, device)\u001b[0m\n\u001b[1;32m    104\u001b[0m attn_mask \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mgenerate_square_subsequent_mask(S)\u001b[39m.\u001b[39mto(device)\n\u001b[1;32m    105\u001b[0m padding_mask \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mgenerate_padding_mask(x[\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcat_cols[\u001b[39m0\u001b[39m]])\u001b[39m.\u001b[39mto(device)\n\u001b[0;32m--> 106\u001b[0m embeddings \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtransformer_encoder(embeddings, mask\u001b[39m=\u001b[39mattn_mask, casual\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m, src_key_padding_mask\u001b[39m=\u001b[39mpadding_mask)\n\u001b[1;32m    107\u001b[0m \u001b[39mprint\u001b[39m(embeddings\u001b[39m.\u001b[39mshape)\n\u001b[1;32m    108\u001b[0m embeddings \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mnorm(embeddings)\n",
      "File \u001b[0;32m~/personal/anaconda3/envs/main4/lib/python3.11/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[1;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/personal/anaconda3/envs/main4/lib/python3.11/site-packages/reformer_pytorch/reformer_pytorch.py:713\u001b[0m, in \u001b[0;36mReformer.forward\u001b[0;34m(self, x, **kwargs)\u001b[0m\n\u001b[1;32m    711\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, x, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[1;32m    712\u001b[0m     x \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mcat([x, x], dim \u001b[39m=\u001b[39m \u001b[39m-\u001b[39m\u001b[39m1\u001b[39m)\n\u001b[0;32m--> 713\u001b[0m     x \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlayers(x, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[1;32m    714\u001b[0m     \u001b[39mreturn\u001b[39;00m torch\u001b[39m.\u001b[39mstack(x\u001b[39m.\u001b[39mchunk(\u001b[39m2\u001b[39m, dim\u001b[39m=\u001b[39m\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m))\u001b[39m.\u001b[39mmean(dim\u001b[39m=\u001b[39m\u001b[39m0\u001b[39m)\n",
      "File \u001b[0;32m~/personal/anaconda3/envs/main4/lib/python3.11/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[1;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/personal/anaconda3/envs/main4/lib/python3.11/site-packages/reformer_pytorch/reversible.py:162\u001b[0m, in \u001b[0;36mReversibleSequence.forward\u001b[0;34m(self, x, arg_route, **kwargs)\u001b[0m\n\u001b[1;32m    159\u001b[0m         x \u001b[39m=\u001b[39m block(x, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mblock_kwargs)\n\u001b[1;32m    160\u001b[0m     \u001b[39mreturn\u001b[39;00m x\n\u001b[0;32m--> 162\u001b[0m \u001b[39mreturn\u001b[39;00m _ReversibleFunction\u001b[39m.\u001b[39mapply(x, blocks, block_kwargs)\n",
      "File \u001b[0;32m~/personal/anaconda3/envs/main4/lib/python3.11/site-packages/torch/autograd/function.py:506\u001b[0m, in \u001b[0;36mFunction.apply\u001b[0;34m(cls, *args, **kwargs)\u001b[0m\n\u001b[1;32m    503\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m torch\u001b[39m.\u001b[39m_C\u001b[39m.\u001b[39m_are_functorch_transforms_active():\n\u001b[1;32m    504\u001b[0m     \u001b[39m# See NOTE: [functorch vjp and autograd interaction]\u001b[39;00m\n\u001b[1;32m    505\u001b[0m     args \u001b[39m=\u001b[39m _functorch\u001b[39m.\u001b[39mutils\u001b[39m.\u001b[39munwrap_dead_wrappers(args)\n\u001b[0;32m--> 506\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39msuper\u001b[39m()\u001b[39m.\u001b[39mapply(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)  \u001b[39m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m    508\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mcls\u001b[39m\u001b[39m.\u001b[39msetup_context \u001b[39m==\u001b[39m _SingleLevelFunction\u001b[39m.\u001b[39msetup_context:\n\u001b[1;32m    509\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mRuntimeError\u001b[39;00m(\n\u001b[1;32m    510\u001b[0m         \u001b[39m'\u001b[39m\u001b[39mIn order to use an autograd.Function with functorch transforms \u001b[39m\u001b[39m'\u001b[39m\n\u001b[1;32m    511\u001b[0m         \u001b[39m'\u001b[39m\u001b[39m(vmap, grad, jvp, jacrev, ...), it must override the setup_context \u001b[39m\u001b[39m'\u001b[39m\n\u001b[1;32m    512\u001b[0m         \u001b[39m'\u001b[39m\u001b[39mstaticmethod. For more details, please see \u001b[39m\u001b[39m'\u001b[39m\n\u001b[1;32m    513\u001b[0m         \u001b[39m'\u001b[39m\u001b[39mhttps://pytorch.org/docs/master/notes/extending.func.html\u001b[39m\u001b[39m'\u001b[39m)\n",
      "File \u001b[0;32m~/personal/anaconda3/envs/main4/lib/python3.11/site-packages/reformer_pytorch/reversible.py:123\u001b[0m, in \u001b[0;36m_ReversibleFunction.forward\u001b[0;34m(ctx, x, blocks, kwargs)\u001b[0m\n\u001b[1;32m    121\u001b[0m ctx\u001b[39m.\u001b[39mkwargs \u001b[39m=\u001b[39m kwargs\n\u001b[1;32m    122\u001b[0m \u001b[39mfor\u001b[39;00m block \u001b[39min\u001b[39;00m blocks:\n\u001b[0;32m--> 123\u001b[0m     x \u001b[39m=\u001b[39m block(x, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[1;32m    124\u001b[0m ctx\u001b[39m.\u001b[39my \u001b[39m=\u001b[39m x\u001b[39m.\u001b[39mdetach()\n\u001b[1;32m    125\u001b[0m ctx\u001b[39m.\u001b[39mblocks \u001b[39m=\u001b[39m blocks\n",
      "File \u001b[0;32m~/personal/anaconda3/envs/main4/lib/python3.11/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[1;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/personal/anaconda3/envs/main4/lib/python3.11/site-packages/reformer_pytorch/reversible.py:59\u001b[0m, in \u001b[0;36mReversibleBlock.forward\u001b[0;34m(self, x, f_args, g_args)\u001b[0m\n\u001b[1;32m     56\u001b[0m     f_args[\u001b[39m'\u001b[39m\u001b[39m_depth\u001b[39m\u001b[39m'\u001b[39m] \u001b[39m=\u001b[39m g_args[\u001b[39m'\u001b[39m\u001b[39m_depth\u001b[39m\u001b[39m'\u001b[39m] \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdepth\n\u001b[1;32m     58\u001b[0m \u001b[39mwith\u001b[39;00m torch\u001b[39m.\u001b[39mno_grad():\n\u001b[0;32m---> 59\u001b[0m     y1 \u001b[39m=\u001b[39m x1 \u001b[39m+\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mf(x2, record_rng\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtraining, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mf_args)\n\u001b[1;32m     60\u001b[0m     y2 \u001b[39m=\u001b[39m x2 \u001b[39m+\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mg(y1, record_rng\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtraining, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mg_args)\n\u001b[1;32m     62\u001b[0m \u001b[39mreturn\u001b[39;00m torch\u001b[39m.\u001b[39mcat([y1, y2], dim\u001b[39m=\u001b[39m\u001b[39m2\u001b[39m)\n",
      "File \u001b[0;32m~/personal/anaconda3/envs/main4/lib/python3.11/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[1;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/personal/anaconda3/envs/main4/lib/python3.11/site-packages/reformer_pytorch/reversible.py:27\u001b[0m, in \u001b[0;36mDeterministic.forward\u001b[0;34m(self, record_rng, set_rng, *args, **kwargs)\u001b[0m\n\u001b[1;32m     24\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mrecord_rng(\u001b[39m*\u001b[39margs)\n\u001b[1;32m     26\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m set_rng:\n\u001b[0;32m---> 27\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mnet(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[1;32m     29\u001b[0m rng_devices \u001b[39m=\u001b[39m []\n\u001b[1;32m     30\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcuda_in_fwd:\n",
      "File \u001b[0;32m~/personal/anaconda3/envs/main4/lib/python3.11/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[1;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/personal/anaconda3/envs/main4/lib/python3.11/site-packages/reformer_pytorch/reformer_pytorch.py:159\u001b[0m, in \u001b[0;36mPreNorm.forward\u001b[0;34m(self, x, **kwargs)\u001b[0m\n\u001b[1;32m    158\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, x, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[0;32m--> 159\u001b[0m     x \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mnorm(x)\n\u001b[1;32m    160\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mfn(x, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m~/personal/anaconda3/envs/main4/lib/python3.11/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[1;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/personal/anaconda3/envs/main4/lib/python3.11/site-packages/torch/nn/modules/normalization.py:190\u001b[0m, in \u001b[0;36mLayerNorm.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    189\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39minput\u001b[39m: Tensor) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Tensor:\n\u001b[0;32m--> 190\u001b[0m     \u001b[39mreturn\u001b[39;00m F\u001b[39m.\u001b[39mlayer_norm(\n\u001b[1;32m    191\u001b[0m         \u001b[39minput\u001b[39m, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mnormalized_shape, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mweight, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbias, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39meps)\n",
      "File \u001b[0;32m~/personal/anaconda3/envs/main4/lib/python3.11/site-packages/torch/nn/functional.py:2515\u001b[0m, in \u001b[0;36mlayer_norm\u001b[0;34m(input, normalized_shape, weight, bias, eps)\u001b[0m\n\u001b[1;32m   2511\u001b[0m \u001b[39mif\u001b[39;00m has_torch_function_variadic(\u001b[39minput\u001b[39m, weight, bias):\n\u001b[1;32m   2512\u001b[0m     \u001b[39mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m   2513\u001b[0m         layer_norm, (\u001b[39minput\u001b[39m, weight, bias), \u001b[39minput\u001b[39m, normalized_shape, weight\u001b[39m=\u001b[39mweight, bias\u001b[39m=\u001b[39mbias, eps\u001b[39m=\u001b[39meps\n\u001b[1;32m   2514\u001b[0m     )\n\u001b[0;32m-> 2515\u001b[0m \u001b[39mreturn\u001b[39;00m torch\u001b[39m.\u001b[39mlayer_norm(\u001b[39minput\u001b[39m, normalized_shape, weight, bias, eps, torch\u001b[39m.\u001b[39mbackends\u001b[39m.\u001b[39mcudnn\u001b[39m.\u001b[39menabled)\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Given normalized_shape=[1024], expected input with shape [*, 1024], but got input of size[32, 100, 64]"
     ]
    }
   ],
   "source": [
    "transformer(next(iter(train_loader)),device='cuda')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[49], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m train_model(\n\u001b[1;32m      2\u001b[0m     transformer, \n\u001b[1;32m      3\u001b[0m     optimizer,\n\u001b[1;32m      4\u001b[0m     {\u001b[39m\"\u001b[39m\u001b[39mtrain\u001b[39m\u001b[39m\"\u001b[39m: train_loader, \u001b[39m\"\u001b[39m\u001b[39mval\u001b[39m\u001b[39m\"\u001b[39m: val_loader, \u001b[39m\"\u001b[39m\u001b[39mtest\u001b[39m\u001b[39m\"\u001b[39m: test_loader},\n\u001b[1;32m      5\u001b[0m     \u001b[39m20\u001b[39m\n\u001b[1;32m      6\u001b[0m )\n",
      "Cell \u001b[0;32mIn[7], line 99\u001b[0m, in \u001b[0;36mtrain_model\u001b[0;34m(model, optimizer, dataloaders, n_epochs, warmup, device, save_path)\u001b[0m\n\u001b[1;32m     97\u001b[0m \u001b[39mfor\u001b[39;00m epoch \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(n_epochs):\n\u001b[1;32m     98\u001b[0m     train_start \u001b[39m=\u001b[39m time\u001b[39m.\u001b[39mperf_counter()\n\u001b[0;32m---> 99\u001b[0m     train_loss, train_metrics \u001b[39m=\u001b[39m train_epoch(model, optimizer, dataloaders[\u001b[39m\"\u001b[39m\u001b[39mtrain\u001b[39m\u001b[39m\"\u001b[39m], warmup, device)\n\u001b[1;32m    100\u001b[0m     train_end \u001b[39m=\u001b[39m time\u001b[39m.\u001b[39mperf_counter()\n\u001b[1;32m    101\u001b[0m     val_loss, val_metrics \u001b[39m=\u001b[39m eval_epoch(model, dataloaders[\u001b[39m\"\u001b[39m\u001b[39mval\u001b[39m\u001b[39m\"\u001b[39m], warmup, device)\n",
      "Cell \u001b[0;32mIn[7], line 39\u001b[0m, in \u001b[0;36mtrain_epoch\u001b[0;34m(model, optimizer, dataloader, warmup, device)\u001b[0m\n\u001b[1;32m     36\u001b[0m     metrics[key][\u001b[39m\"\u001b[39m\u001b[39maccuracy\u001b[39m\u001b[39m\"\u001b[39m]\u001b[39m.\u001b[39mupdate(y_pred, y\u001b[39m.\u001b[39mto(\u001b[39m\"\u001b[39m\u001b[39mcpu\u001b[39m\u001b[39m\"\u001b[39m))\n\u001b[1;32m     38\u001b[0m optimizer\u001b[39m.\u001b[39mzero_grad()\n\u001b[0;32m---> 39\u001b[0m loss\u001b[39m.\u001b[39mbackward()\n\u001b[1;32m     40\u001b[0m optimizer\u001b[39m.\u001b[39mstep()\n\u001b[1;32m     42\u001b[0m cur_count \u001b[39m=\u001b[39m  torch\u001b[39m.\u001b[39msum((y \u001b[39m!=\u001b[39m \u001b[39m0\u001b[39m)\u001b[39m.\u001b[39mfloat())\u001b[39m.\u001b[39mitem()\n",
      "File \u001b[0;32m~/personal/anaconda3/envs/main4/lib/python3.11/site-packages/torch/_tensor.py:487\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    477\u001b[0m \u001b[39mif\u001b[39;00m has_torch_function_unary(\u001b[39mself\u001b[39m):\n\u001b[1;32m    478\u001b[0m     \u001b[39mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    479\u001b[0m         Tensor\u001b[39m.\u001b[39mbackward,\n\u001b[1;32m    480\u001b[0m         (\u001b[39mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    485\u001b[0m         inputs\u001b[39m=\u001b[39minputs,\n\u001b[1;32m    486\u001b[0m     )\n\u001b[0;32m--> 487\u001b[0m torch\u001b[39m.\u001b[39mautograd\u001b[39m.\u001b[39mbackward(\n\u001b[1;32m    488\u001b[0m     \u001b[39mself\u001b[39m, gradient, retain_graph, create_graph, inputs\u001b[39m=\u001b[39minputs\n\u001b[1;32m    489\u001b[0m )\n",
      "File \u001b[0;32m~/personal/anaconda3/envs/main4/lib/python3.11/site-packages/torch/autograd/__init__.py:200\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    195\u001b[0m     retain_graph \u001b[39m=\u001b[39m create_graph\n\u001b[1;32m    197\u001b[0m \u001b[39m# The reason we repeat same the comment below is that\u001b[39;00m\n\u001b[1;32m    198\u001b[0m \u001b[39m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    199\u001b[0m \u001b[39m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 200\u001b[0m Variable\u001b[39m.\u001b[39m_execution_engine\u001b[39m.\u001b[39mrun_backward(  \u001b[39m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[1;32m    201\u001b[0m     tensors, grad_tensors_, retain_graph, create_graph, inputs,\n\u001b[1;32m    202\u001b[0m     allow_unreachable\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m, accumulate_grad\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "train_model(\n",
    "    transformer, \n",
    "    optimizer,\n",
    "    {\"train\": train_loader, \"val\": val_loader, \"test\": test_loader},\n",
    "    20\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = df.sort_values(\"TRDATETIME\").groupby(\"client_id\").apply(lambda x: x[\"amount_rur_bin\"] == x[\"amount_rur_bin\"].shift())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.17164274952957415"
      ]
     },
     "execution_count": 111,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a.reset_index()[\"amount_rur_bin\"].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = {\n",
    "    \"experiment_name\": \"run2\",\n",
    "    \"dataset\": \"rosbank\",\n",
    "    \"min_length\": 20,\n",
    "    \"max_length\": 100,\n",
    "    \"batch_size\": 32,\n",
    "    \"type\": \"transformer\",\n",
    "    \"transformer_params\": {\n",
    "        \"feature_embeddings\": {\"small_group\": (345, 64), \"amount_rur_bin\": (11, 64)}, \n",
    "        \"linear_proj\": 64,\n",
    "        \"n_head\": 8, \n",
    "        \"dim_feedforward\": 128, \n",
    "        \"dropout\": 0.1, \n",
    "        \"num_layers\": 6, \n",
    "        \"head_hidden\": 128,\n",
    "    },\n",
    "    \"lr\": 1e-3,\n",
    "    \"n_epochs\": 50,\n",
    "    \"warmup\": 10,\n",
    "    \"device\": \"cuda\",\n",
    "    \"save_path\": \"./\"\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "with open(\"config.json\", \"w\") as f:\n",
    "    json.dump(config, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "from yaml import load, dump\n",
    "# from yaml import CLoader as Loader, CDumper as Dumper\n",
    "\n",
    "with open(\"config.yaml\", \"w\") as f:\n",
    "    dump(config, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'././'"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.path.exists(os.path.join(\"./\", \"./\"))\n",
    "os.path.join(\"./\", \"./\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "load() missing 1 required positional argument: 'Loader'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[59], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[39mwith\u001b[39;00m \u001b[39mopen\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39mconfig.yaml\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mr\u001b[39m\u001b[39m\"\u001b[39m) \u001b[39mas\u001b[39;00m f:\n\u001b[1;32m----> 2\u001b[0m     d \u001b[39m=\u001b[39m load(f)\n\u001b[0;32m      4\u001b[0m d\n",
      "\u001b[1;31mTypeError\u001b[0m: load() missing 1 required positional argument: 'Loader'"
     ]
    }
   ],
   "source": [
    "\n",
    "with open(\"config.yaml\", \"r\") as f:\n",
    "    d = load(f)\n",
    "\n",
    "d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main(config):\n",
    "    wandb.login()\n",
    "\n",
    "    wandb.init(\n",
    "        project=\"deep-learning-project\",\n",
    "        name=config[\"experiment_name\"], \n",
    "        tags=config[\"tags\"],\n",
    "        config=config\n",
    "    )\n",
    "\n",
    "    if config[\"dataset\"] == \"rosbank\":\n",
    "        df = pd.read_csv('data/rosbank/train.csv')\n",
    "        df['TRDATETIME'] = pd.to_datetime(df['TRDATETIME'], format='%d%b%y:%H:%M:%S')\n",
    "        df = df.rename(columns={'cl_id':'client_id', 'MCC':'small_group', 'amount':'amount_rur'})\n",
    "        \n",
    "        mcc_to_id = {mcc: i+1 for i, mcc in enumerate(df['small_group'].unique())}\n",
    "\n",
    "        df['amount_rur_bin'] = 1 + KBinsDiscretizer(10, encode='ordinal', subsample=None).fit_transform(df[['amount_rur']]).astype('int')\n",
    "        df['small_group'] = df['small_group'].map(mcc_to_id)\n",
    "\n",
    "    else:\n",
    "        pass\n",
    "\n",
    "    clients_train, clients_val_test = train_test_split(df[\"client_id\"].unique(), test_size=0.2, random_state=42)\n",
    "    clients_val, clients_test = train_test_split(clients_val_test, test_size=0.5, random_state=42)\n",
    "\n",
    "    train_ds = TransactionDataset(\n",
    "        df[lambda x: x[\"client_id\"].isin(clients_train)], \n",
    "        id_col=\"client_id\", \n",
    "        dt_col=\"TRDATETIME\", \n",
    "        cat_cols=[\"small_group\", \"amount_rur_bin\"],\n",
    "        min_length=config[\"min_length\"],\n",
    "        max_length=config[\"max_length\"],\n",
    "        random_slice=True\n",
    "    )\n",
    "\n",
    "    val_ds = TransactionDataset(\n",
    "        df[lambda x: x[\"client_id\"].isin(clients_val)], \n",
    "        id_col=\"client_id\", \n",
    "        dt_col=\"TRDATETIME\", \n",
    "        cat_cols=[\"small_group\", \"amount_rur_bin\"],\n",
    "        min_length=config[\"min_length\"],\n",
    "        max_length=config[\"max_length\"],\n",
    "        random_slice=False\n",
    "    )\n",
    "\n",
    "    test_ds = TransactionDataset(\n",
    "        df[lambda x: x[\"client_id\"].isin(clients_test)], \n",
    "        id_col=\"client_id\", \n",
    "        dt_col=\"TRDATETIME\", \n",
    "        cat_cols=[\"small_group\", \"amount_rur_bin\"],\n",
    "        min_length=config[\"min_length\"],\n",
    "        max_length=config[\"max_length\"],\n",
    "        random_slice=False\n",
    "    )\n",
    "\n",
    "    train_loader = DataLoader(train_ds, batch_size=config[\"batch_size\"], shuffle=True, collate_fn=partial(transaction_collate_fn,100))\n",
    "    val_loader = DataLoader(val_ds, batch_size=config[\"batch_size\"], shuffle=False, collate_fn=partial(transaction_collate_fn,100))\n",
    "    test_loader = DataLoader(test_ds, batch_size=config[\"batch_size\"], shuffle=False, collate_fn=partial(transaction_collate_fn,100))\n",
    "\n",
    "    transformer = TransformerModel(**config[\"transformer_params\"])\n",
    "    optimizer = torch.optim.Adam(transformer.parameters(), lr=config[\"lr\"])\n",
    "\n",
    "    train_model(\n",
    "        transformer, \n",
    "        optimizer, \n",
    "        {\"train\": train_loader, \"val\": val_loader, \"test\": test_loader}, \n",
    "        n_epochs=config[\"n_epochs\"],\n",
    "        warmup=config[\"warmup\"],\n",
    "        device=config[\"device\"],\n",
    "        save_path=config[\"save_path\"]\n",
    "    )\n",
    "\n",
    "    wandb.finish()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Calling wandb.login() after wandb.init() has no effect.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Finishing last run (ID:bu3ggt4d) before initializing another..."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a3d4f01021c4472eafce6656a6a5a0b8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.001 MB of 0.001 MB uploaded (0.000 MB deduped)\\r'), FloatProgress(value=1.0, max…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">run2</strong> at: <a href='https://wandb.ai/aayugay99/deep-learning-project/runs/bu3ggt4d' target=\"_blank\">https://wandb.ai/aayugay99/deep-learning-project/runs/bu3ggt4d</a><br/>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>.\\wandb\\run-20230522_184446-bu3ggt4d\\logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Successfully finished last run (ID:bu3ggt4d). Initializing new run:<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "69ac9dc73049497f8e49d12a39acf682",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='Waiting for wandb.init()...\\r'), FloatProgress(value=0.016666666666666666, max=1.0…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.15.3 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.14.0"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>d:\\Skoltech\\Deep Learning\\dl2023\\efficient_transformers_on_transactional_data\\wandb\\run-20230522_184723-cgbc8t2b</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/aayugay99/deep-learning-project/runs/cgbc8t2b' target=\"_blank\">run2</a></strong> to <a href='https://wandb.ai/aayugay99/deep-learning-project' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/aayugay99/deep-learning-project' target=\"_blank\">https://wandb.ai/aayugay99/deep-learning-project</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/aayugay99/deep-learning-project/runs/cgbc8t2b' target=\"_blank\">https://wandb.ai/aayugay99/deep-learning-project/runs/cgbc8t2b</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "628 sequences were filtered\n",
      "77 sequences were filtered\n",
      "70 sequences were filtered\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\User\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\torch\\nn\\modules\\transformer.py:562: UserWarning: Converting mask without torch.bool dtype to bool; this will negatively affect performance. Prefer to use a boolean mask directly. (Triggered internally at C:\\actions-runner\\_work\\pytorch\\pytorch\\builder\\windows\\pytorch\\aten\\src\\ATen\\native\\transformers\\attention.cpp:152.)\n",
      "  return torch._transformer_encoder_layer_fwd(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>Epoch</td><td>▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇███</td></tr><tr><td>Train loss</td><td>█▅▄▃▃▃▃▃▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>Train time</td><td>█▅▂▁▁▁▂▁▁▁▁▁▂▁▁▂▁▂▁▁▁▁▂▁▂▁▁▁▁▂▂▁▂▂▂▂▁▁▁▁</td></tr><tr><td>Val loss</td><td>█▅▄▄▃▂▂▂▂▁▁▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▂▂▂▁▂</td></tr><tr><td>Val time</td><td>▂▃▁▃▂▁▁▁▁▁▃▃▂▁▁▂▁▁▁▁▁▁▁█▁▁▂▁▂█▂▃▂▃▂▆▁▁▃▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>Epoch</td><td>50</td></tr><tr><td>Test loss</td><td>4.94315</td></tr><tr><td>Test time</td><td>0.19568</td></tr><tr><td>Train loss</td><td>4.75581</td></tr><tr><td>Train time</td><td>3.7141</td></tr><tr><td>Val loss</td><td>5.037</td></tr><tr><td>Val time</td><td>0.19415</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">run2</strong> at: <a href='https://wandb.ai/aayugay99/deep-learning-project/runs/cgbc8t2b' target=\"_blank\">https://wandb.ai/aayugay99/deep-learning-project/runs/cgbc8t2b</a><br/>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>.\\wandb\\run-20230522_184723-cgbc8t2b\\logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "main(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wandb.login()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "cf92aa13fedf815d5c8dd192b8d835913fde3e8bc926b2a0ad6cc74ef2ba3ca2"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
