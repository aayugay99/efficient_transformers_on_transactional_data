{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import KBinsDiscretizer\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn \n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "\n",
    "from dataset import TransactionDataset, transaction_collate_fn\n",
    "\n",
    "from torchmetrics.classification import MulticlassF1Score, Accuracy\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import time\n",
    "\n",
    "import wandb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>PERIOD</th>\n",
       "      <th>client_id</th>\n",
       "      <th>small_group</th>\n",
       "      <th>channel_type</th>\n",
       "      <th>currency</th>\n",
       "      <th>TRDATETIME</th>\n",
       "      <th>amount_rur</th>\n",
       "      <th>trx_category</th>\n",
       "      <th>target_flag</th>\n",
       "      <th>target_sum</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>01/10/2017</td>\n",
       "      <td>0</td>\n",
       "      <td>5200</td>\n",
       "      <td>NaN</td>\n",
       "      <td>810</td>\n",
       "      <td>2017-10-21 00:00:00</td>\n",
       "      <td>5023.0</td>\n",
       "      <td>POS</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>01/10/2017</td>\n",
       "      <td>0</td>\n",
       "      <td>6011</td>\n",
       "      <td>NaN</td>\n",
       "      <td>810</td>\n",
       "      <td>2017-10-12 12:24:07</td>\n",
       "      <td>20000.0</td>\n",
       "      <td>DEPOSIT</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>01/12/2017</td>\n",
       "      <td>0</td>\n",
       "      <td>5921</td>\n",
       "      <td>NaN</td>\n",
       "      <td>810</td>\n",
       "      <td>2017-12-05 00:00:00</td>\n",
       "      <td>767.0</td>\n",
       "      <td>POS</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>01/10/2017</td>\n",
       "      <td>0</td>\n",
       "      <td>5411</td>\n",
       "      <td>NaN</td>\n",
       "      <td>810</td>\n",
       "      <td>2017-10-21 00:00:00</td>\n",
       "      <td>2031.0</td>\n",
       "      <td>POS</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>01/10/2017</td>\n",
       "      <td>0</td>\n",
       "      <td>6012</td>\n",
       "      <td>NaN</td>\n",
       "      <td>810</td>\n",
       "      <td>2017-10-24 13:14:24</td>\n",
       "      <td>36562.0</td>\n",
       "      <td>C2C_OUT</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       PERIOD  client_id  small_group channel_type  currency  \\\n",
       "0  01/10/2017          0         5200          NaN       810   \n",
       "1  01/10/2017          0         6011          NaN       810   \n",
       "2  01/12/2017          0         5921          NaN       810   \n",
       "3  01/10/2017          0         5411          NaN       810   \n",
       "4  01/10/2017          0         6012          NaN       810   \n",
       "\n",
       "           TRDATETIME  amount_rur trx_category  target_flag  target_sum  \n",
       "0 2017-10-21 00:00:00      5023.0          POS            0         0.0  \n",
       "1 2017-10-12 12:24:07     20000.0      DEPOSIT            0         0.0  \n",
       "2 2017-12-05 00:00:00       767.0          POS            0         0.0  \n",
       "3 2017-10-21 00:00:00      2031.0          POS            0         0.0  \n",
       "4 2017-10-24 13:14:24     36562.0      C2C_OUT            0         0.0  "
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('data/rosbank/train.csv')\n",
    "df['TRDATETIME'] = pd.to_datetime(df['TRDATETIME'], format='%d%b%y:%H:%M:%S')\n",
    "df = df.rename(columns={'cl_id':'client_id', 'MCC':'small_group', 'amount':'amount_rur'})\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df = pd.read_csv('data/sberbank/transactions_train.csv')\n",
    "# df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "mcc_to_id = {mcc: i+1 for i, mcc in enumerate(df['small_group'].unique())}\n",
    "\n",
    "df['amount_rur_bin'] = 1 + KBinsDiscretizer(10, encode='ordinal', subsample=None).fit_transform(df[['amount_rur']]).astype('int')\n",
    "df['small_group'] = df['small_group'].map(mcc_to_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>PERIOD</th>\n",
       "      <th>client_id</th>\n",
       "      <th>small_group</th>\n",
       "      <th>channel_type</th>\n",
       "      <th>currency</th>\n",
       "      <th>TRDATETIME</th>\n",
       "      <th>amount_rur</th>\n",
       "      <th>trx_category</th>\n",
       "      <th>target_flag</th>\n",
       "      <th>target_sum</th>\n",
       "      <th>amount_rur_bin</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>01/10/2017</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>810</td>\n",
       "      <td>2017-10-21 00:00:00</td>\n",
       "      <td>5023.00</td>\n",
       "      <td>POS</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>01/10/2017</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>NaN</td>\n",
       "      <td>810</td>\n",
       "      <td>2017-10-12 12:24:07</td>\n",
       "      <td>20000.00</td>\n",
       "      <td>DEPOSIT</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>01/12/2017</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>NaN</td>\n",
       "      <td>810</td>\n",
       "      <td>2017-12-05 00:00:00</td>\n",
       "      <td>767.00</td>\n",
       "      <td>POS</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>01/10/2017</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>NaN</td>\n",
       "      <td>810</td>\n",
       "      <td>2017-10-21 00:00:00</td>\n",
       "      <td>2031.00</td>\n",
       "      <td>POS</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>01/10/2017</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>NaN</td>\n",
       "      <td>810</td>\n",
       "      <td>2017-10-24 13:14:24</td>\n",
       "      <td>36562.00</td>\n",
       "      <td>C2C_OUT</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>490508</th>\n",
       "      <td>01/04/2017</td>\n",
       "      <td>10176</td>\n",
       "      <td>2</td>\n",
       "      <td>type1</td>\n",
       "      <td>810</td>\n",
       "      <td>2017-04-24 14:05:26</td>\n",
       "      <td>600.00</td>\n",
       "      <td>WD_ATM_ROS</td>\n",
       "      <td>1</td>\n",
       "      <td>405.0</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>490509</th>\n",
       "      <td>01/06/2017</td>\n",
       "      <td>10171</td>\n",
       "      <td>4</td>\n",
       "      <td>type1</td>\n",
       "      <td>810</td>\n",
       "      <td>2017-06-06 00:00:00</td>\n",
       "      <td>132.00</td>\n",
       "      <td>POS</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>490510</th>\n",
       "      <td>01/02/2017</td>\n",
       "      <td>10167</td>\n",
       "      <td>51</td>\n",
       "      <td>type1</td>\n",
       "      <td>810</td>\n",
       "      <td>2017-02-03 00:00:00</td>\n",
       "      <td>1000.00</td>\n",
       "      <td>POS</td>\n",
       "      <td>1</td>\n",
       "      <td>280428.2</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>490511</th>\n",
       "      <td>01/06/2017</td>\n",
       "      <td>10163</td>\n",
       "      <td>39</td>\n",
       "      <td>type1</td>\n",
       "      <td>810</td>\n",
       "      <td>2017-06-08 00:00:00</td>\n",
       "      <td>100.00</td>\n",
       "      <td>POS</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>490512</th>\n",
       "      <td>01/06/2017</td>\n",
       "      <td>10162</td>\n",
       "      <td>4</td>\n",
       "      <td>type1</td>\n",
       "      <td>810</td>\n",
       "      <td>2017-06-15 00:00:00</td>\n",
       "      <td>441.33</td>\n",
       "      <td>POS</td>\n",
       "      <td>1</td>\n",
       "      <td>253.0</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>490513 rows × 11 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            PERIOD  client_id  small_group channel_type  currency  \\\n",
       "0       01/10/2017          0            1          NaN       810   \n",
       "1       01/10/2017          0            2          NaN       810   \n",
       "2       01/12/2017          0            3          NaN       810   \n",
       "3       01/10/2017          0            4          NaN       810   \n",
       "4       01/10/2017          0            5          NaN       810   \n",
       "...            ...        ...          ...          ...       ...   \n",
       "490508  01/04/2017      10176            2        type1       810   \n",
       "490509  01/06/2017      10171            4        type1       810   \n",
       "490510  01/02/2017      10167           51        type1       810   \n",
       "490511  01/06/2017      10163           39        type1       810   \n",
       "490512  01/06/2017      10162            4        type1       810   \n",
       "\n",
       "                TRDATETIME  amount_rur trx_category  target_flag  target_sum  \\\n",
       "0      2017-10-21 00:00:00     5023.00          POS            0         0.0   \n",
       "1      2017-10-12 12:24:07    20000.00      DEPOSIT            0         0.0   \n",
       "2      2017-12-05 00:00:00      767.00          POS            0         0.0   \n",
       "3      2017-10-21 00:00:00     2031.00          POS            0         0.0   \n",
       "4      2017-10-24 13:14:24    36562.00      C2C_OUT            0         0.0   \n",
       "...                    ...         ...          ...          ...         ...   \n",
       "490508 2017-04-24 14:05:26      600.00   WD_ATM_ROS            1       405.0   \n",
       "490509 2017-06-06 00:00:00      132.00          POS            0         0.0   \n",
       "490510 2017-02-03 00:00:00     1000.00          POS            1    280428.2   \n",
       "490511 2017-06-08 00:00:00      100.00          POS            0         0.0   \n",
       "490512 2017-06-15 00:00:00      441.33          POS            1       253.0   \n",
       "\n",
       "        amount_rur_bin  \n",
       "0                    9  \n",
       "1                   10  \n",
       "2                    6  \n",
       "3                    8  \n",
       "4                   10  \n",
       "...                ...  \n",
       "490508               5  \n",
       "490509               2  \n",
       "490510               7  \n",
       "490511               2  \n",
       "490512               5  \n",
       "\n",
       "[490513 rows x 11 columns]"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df = df.rename({'trans_date': 'TRDATETIME'}, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "628 sequences were filtered\n",
      "77 sequences were filtered\n",
      "70 sequences were filtered\n"
     ]
    }
   ],
   "source": [
    "clients_train, clients_val_test = train_test_split(df[\"client_id\"].unique(), test_size=0.2, random_state=42)\n",
    "clients_val, clients_test = train_test_split(clients_val_test, test_size=0.5, random_state=42)\n",
    "\n",
    "train_ds = TransactionDataset(\n",
    "    df[lambda x: x[\"client_id\"].isin(clients_train)], \n",
    "    id_col=\"client_id\", \n",
    "    dt_col=\"TRDATETIME\", \n",
    "    cat_cols=[\"small_group\", \"amount_rur_bin\"],\n",
    "    min_length=20,\n",
    "    max_length=100\n",
    ")\n",
    "\n",
    "val_ds = TransactionDataset(\n",
    "    df[lambda x: x[\"client_id\"].isin(clients_val)], \n",
    "    id_col=\"client_id\", \n",
    "    dt_col=\"TRDATETIME\", \n",
    "    cat_cols=[\"small_group\", \"amount_rur_bin\"],\n",
    "    min_length=20,\n",
    "    max_length=100\n",
    ")\n",
    "\n",
    "test_ds = TransactionDataset(\n",
    "    df[lambda x: x[\"client_id\"].isin(clients_test)], \n",
    "    id_col=\"client_id\", \n",
    "    dt_col=\"TRDATETIME\", \n",
    "    cat_cols=[\"small_group\", \"amount_rur_bin\"],\n",
    "    min_length=20,\n",
    "    max_length=100\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting performer-pytorch\n",
      "  Downloading performer_pytorch-1.1.4-py3-none-any.whl (13 kB)\n",
      "Requirement already satisfied: einops>=0.3 in /home/alex/personal/anaconda3/envs/main4/lib/python3.11/site-packages (from performer-pytorch) (0.6.1)\n",
      "Requirement already satisfied: local-attention>=1.1.1 in /home/alex/personal/anaconda3/envs/main4/lib/python3.11/site-packages (from performer-pytorch) (1.8.6)\n",
      "Requirement already satisfied: torch>=1.6 in /home/alex/personal/anaconda3/envs/main4/lib/python3.11/site-packages (from performer-pytorch) (2.0.0+cu118)\n",
      "Requirement already satisfied: axial-positional-embedding>=0.1.0 in /home/alex/personal/anaconda3/envs/main4/lib/python3.11/site-packages (from performer-pytorch) (0.2.1)\n",
      "Requirement already satisfied: filelock in /home/alex/personal/anaconda3/envs/main4/lib/python3.11/site-packages (from torch>=1.6->performer-pytorch) (3.9.0)\n",
      "Requirement already satisfied: typing-extensions in /home/alex/personal/anaconda3/envs/main4/lib/python3.11/site-packages (from torch>=1.6->performer-pytorch) (4.4.0)\n",
      "Requirement already satisfied: sympy in /home/alex/personal/anaconda3/envs/main4/lib/python3.11/site-packages (from torch>=1.6->performer-pytorch) (1.11.1)\n",
      "Requirement already satisfied: networkx in /home/alex/personal/anaconda3/envs/main4/lib/python3.11/site-packages (from torch>=1.6->performer-pytorch) (3.0)\n",
      "Requirement already satisfied: jinja2 in /home/alex/personal/anaconda3/envs/main4/lib/python3.11/site-packages (from torch>=1.6->performer-pytorch) (3.1.2)\n",
      "Requirement already satisfied: triton==2.0.0 in /home/alex/personal/anaconda3/envs/main4/lib/python3.11/site-packages (from torch>=1.6->performer-pytorch) (2.0.0)\n",
      "Requirement already satisfied: cmake in /home/alex/personal/anaconda3/envs/main4/lib/python3.11/site-packages (from triton==2.0.0->torch>=1.6->performer-pytorch) (3.25.0)\n",
      "Requirement already satisfied: lit in /home/alex/personal/anaconda3/envs/main4/lib/python3.11/site-packages (from triton==2.0.0->torch>=1.6->performer-pytorch) (15.0.7)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /home/alex/personal/anaconda3/envs/main4/lib/python3.11/site-packages (from jinja2->torch>=1.6->performer-pytorch) (2.1.1)\n",
      "Requirement already satisfied: mpmath>=0.19 in /home/alex/personal/anaconda3/envs/main4/lib/python3.11/site-packages (from sympy->torch>=1.6->performer-pytorch) (1.2.1)\n",
      "Installing collected packages: performer-pytorch\n",
      "Successfully installed performer-pytorch-1.1.4\n"
     ]
    }
   ],
   "source": [
    "!pip install performer-pytorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "from performer_pytorch import Performer, SelfAttention\n",
    "import copy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransactionEncoder(nn.Module):\n",
    "    def __init__(self, feature_embeddings, linear_proj: int=None):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.feature_embeddings = feature_embeddings\n",
    "        self.embeddings = nn.ModuleDict({key: nn.Embedding(vocab, dim) for key, (vocab, dim) in feature_embeddings.items()})\n",
    "        \n",
    "        if linear_proj is not None:\n",
    "            self.embedding_dim = linear_proj\n",
    "            self.linear_proj = nn.Linear(sum([dim for key, (vocab, dim) in feature_embeddings.items()]), linear_proj)\n",
    "        else:\n",
    "            self.embedding_dim = sum([dim for key, (vocab, dim) in feature_embeddings.items()])\n",
    "            self.linear_proj = nn.Identity()\n",
    "\n",
    "    def forward(self, x: torch.Tensor, device: str=\"cpu\") -> torch.Tensor:\n",
    "        embeddings = [self.embeddings[key](x[key].to(device)) for key in self.feature_embeddings]\n",
    "        proj = self.linear_proj(torch.cat(embeddings, dim=2))\n",
    "        return proj\n",
    "\n",
    "\n",
    "class Head(nn.Module):\n",
    "    def __init__(self, embedding_dim: int, hidden_dim: int, vocab_size: int):\n",
    "        super().__init__()\n",
    "\n",
    "        self.head = nn.Sequential(\n",
    "            nn.Linear(embedding_dim, hidden_dim),\n",
    "            nn.GELU(),\n",
    "            nn.Linear(hidden_dim, vocab_size),\n",
    "        )\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        return self.head(x)\n",
    "\n",
    "\n",
    "class PositionalEncoding(nn.Module):\n",
    "    def __init__(self, embedding_dim: int, dropout: float=0.1, max_len: int=1000):\n",
    "        super().__init__()\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "        position = torch.arange(max_len).unsqueeze(1)\n",
    "        div_term = torch.exp(torch.arange(0, embedding_dim, 2) * (-9.21034037198 / embedding_dim))\n",
    "        pe = torch.zeros(1, max_len, embedding_dim)\n",
    "        pe[0, :, 0::2] = torch.sin(position * div_term)\n",
    "        pe[0, :, 1::2] = torch.cos(position * div_term)\n",
    "        self.register_buffer('pe', pe)\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Arguments:\n",
    "            x: Tensor, shape ``[batch_size, seq_len, embedding_dim]``\n",
    "        \"\"\"\n",
    "        x = x + self.pe[:, :x.size(1), :]\n",
    "        return self.dropout(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Block(nn.Module):\n",
    "    def __init__(\n",
    "            self, \n",
    "            d_model, \n",
    "            dim_feedforward,\n",
    "            dropout,\n",
    "            n_head,\n",
    "            self_attention,\n",
    "            **kwargs\n",
    "            # dim_head,\n",
    "            # local_heads,\n",
    "            # local_window_size,\n",
    "        ):\n",
    "        super().__init__()\n",
    "\n",
    "        self.self_attn = self_attention(\n",
    "            d_model,\n",
    "            heads = n_head,\n",
    "            causal = True,\n",
    "            **kwargs\n",
    "           # dim_head = dim_head,\n",
    "           # local_heads = local_heads,\n",
    "           # local_window_size = local_window_size,\n",
    "        )\n",
    "        \n",
    "        self.linear1 = nn.Linear(d_model, dim_feedforward)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.linear2 = nn.Linear(dim_feedforward, d_model)\n",
    "\n",
    "        self.norm1 = nn.LayerNorm(d_model, eps=1e-5)\n",
    "        self.norm2 = nn.LayerNorm(d_model, eps=1e-5)\n",
    "        self.dropout1 = nn.Dropout(dropout)\n",
    "        self.dropout2 = nn.Dropout(dropout)\n",
    "\n",
    "        self.activation = nn.GELU()\n",
    "\n",
    "    def forward(self, x, input_mask=None):\n",
    "        x = self.norm1(x + self._sa_block(x, input_mask=input_mask))\n",
    "        x = self.norm2(x + self._ff_block(x))\n",
    "        return x\n",
    "\n",
    "    def _sa_block(self, x, input_mask=None):\n",
    "        x = self.self_attn(x, input_mask=input_mask)\n",
    "        return self.dropout1(x)\n",
    "    \n",
    "    def _ff_block(self, x):\n",
    "        x = self.linear2(self.dropout(self.activation(self.linear1(x))))\n",
    "        return self.dropout2(x)\n",
    "    \n",
    "\n",
    "class Encoder(nn.Module):\n",
    "    def __init__(self, block, num_layers):\n",
    "        super().__init__()\n",
    "\n",
    "        self.blocks = nn.ModuleList([copy.deepcopy(block) for i in range(num_layers)])\n",
    "\n",
    "    def forward(self, x, input_mask=None):\n",
    "        for block in self.blocks:\n",
    "            x = block(x, input_mask)\n",
    "        return x\n",
    "\n",
    "\n",
    "class PerformerModel(nn.Module):\n",
    "    def __init__(\n",
    "            self, \n",
    "            feature_embeddings, \n",
    "            linear_proj: int=None,\n",
    "            n_head: int=8, \n",
    "            dropout: float=0.1, \n",
    "            num_layers: int=6, \n",
    "            dim_feedforward: int=128,\n",
    "            head_hidden: int=128,\n",
    "            max_len: int=1000,\n",
    "            dim_head: int=32,\n",
    "            local_heads: int=0,\n",
    "            local_window_size: int=256,\n",
    "        ):\n",
    "        super().__init__()\n",
    "\n",
    "        self.transaction_encoder = TransactionEncoder(feature_embeddings, linear_proj=linear_proj)\n",
    "        self.embedding_dim = self.transaction_encoder.embedding_dim\n",
    "        self.cat_cols = list(feature_embeddings.keys())\n",
    "        self.num_classes_dict = {key: num_classes for key, (num_classes, _) in feature_embeddings.items()}\n",
    "        \n",
    "        self.pos_emb = PositionalEncoding(self.embedding_dim, dropout, max_len)\n",
    "\n",
    "        self.encoder_layer = Block(\n",
    "            self.embedding_dim, \n",
    "            dim_feedforward, \n",
    "            dropout, \n",
    "            n_head, \n",
    "            SelfAttention,\n",
    "            dim_head=dim_head, \n",
    "            local_heads=local_heads, \n",
    "            local_window_size=local_window_size\n",
    "        )\n",
    "        self.transformer_encoder = Encoder(self.encoder_layer, num_layers)\n",
    "        \n",
    "        self.heads = nn.ModuleDict({\n",
    "            key: Head(\n",
    "                self.embedding_dim, \n",
    "                head_hidden, \n",
    "                num_classes\n",
    "            ) for key, num_classes in self.num_classes_dict.items()\n",
    "        })\n",
    "\n",
    "    def forward(self, x: torch.Tensor, device: str=\"cpu\") -> torch.Tensor:\n",
    "        embeddings = self.transaction_encoder(x, device=device)\n",
    "        embeddings = self.pos_emb(embeddings)\n",
    "        \n",
    "        padding_mask = self.generate_padding_mask(x[self.cat_cols[0]]).to(device)\n",
    "        embeddings = self.transformer_encoder(embeddings, input_mask=padding_mask)\n",
    "\n",
    "        logits = {key: self.heads[key](embeddings) for key in self.cat_cols}\n",
    "        return logits\n",
    "    \n",
    "    @staticmethod\n",
    "    def generate_padding_mask(x: torch.Tensor) -> torch.Tensor:\n",
    "        return torch.where(x == 0, True, False).bool()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Block(nn.Module):\n",
    "    def __init__(\n",
    "            self, \n",
    "            d_model, \n",
    "            dim_feedforward,\n",
    "            dropout,\n",
    "            self_attention\n",
    "        ):\n",
    "        super().__init__()\n",
    "\n",
    "        self.self_attn = self_attention\n",
    "        \n",
    "        self.linear1 = nn.Linear(d_model, dim_feedforward)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.linear2 = nn.Linear(dim_feedforward, d_model)\n",
    "\n",
    "        self.norm1 = nn.LayerNorm(d_model, eps=1e-5)\n",
    "        self.norm2 = nn.LayerNorm(d_model, eps=1e-5)\n",
    "        self.dropout1 = nn.Dropout(dropout)\n",
    "        self.dropout2 = nn.Dropout(dropout)\n",
    "\n",
    "        self.activation = nn.GELU()\n",
    "\n",
    "    def forward(self, x, input_mask=None):\n",
    "        x = self.norm1(x + self._sa_block(x, input_mask=input_mask))\n",
    "        x = self.norm2(x + self._ff_block(x))\n",
    "        return x\n",
    "\n",
    "    def _sa_block(self, x, input_mask=None):\n",
    "        x = self.self_attn(x, input_mask=input_mask)\n",
    "        return self.dropout1(x)\n",
    "    \n",
    "    def _ff_block(self, x):\n",
    "        x = self.linear2(self.dropout(self.activation(self.linear1(x))))\n",
    "        return self.dropout2(x)\n",
    "    \n",
    "\n",
    "class Encoder(nn.Module):\n",
    "    def __init__(self, block, num_layers):\n",
    "        super().__init__()\n",
    "\n",
    "        self.blocks = nn.ModuleList([copy.deepcopy(block) for i in range(num_layers)])\n",
    "\n",
    "    def forward(self, x, input_mask=None):\n",
    "        for block in self.blocks:\n",
    "            x = block(x, input_mask)\n",
    "        return x\n",
    "\n",
    "\n",
    "class PerformerModel(nn.Module):\n",
    "    def __init__(\n",
    "            self, \n",
    "            feature_embeddings, \n",
    "            linear_proj: int=None,\n",
    "            n_head: int=8, \n",
    "            dropout: float=0.1, \n",
    "            num_layers: int=6, \n",
    "            dim_feedforward: int=128,\n",
    "            head_hidden: int=128,\n",
    "            max_len: int=1000,\n",
    "            dim_head: int=32,\n",
    "            local_heads: int=0,\n",
    "            local_window_size: int=256,\n",
    "        ):\n",
    "        super().__init__()\n",
    "\n",
    "        self.transaction_encoder = TransactionEncoder(feature_embeddings, linear_proj=linear_proj)\n",
    "        self.embedding_dim = self.transaction_encoder.embedding_dim\n",
    "        self.cat_cols = list(feature_embeddings.keys())\n",
    "        self.num_classes_dict = {key: num_classes for key, (num_classes, _) in feature_embeddings.items()}\n",
    "        \n",
    "        self.pos_emb = PositionalEncoding(self.embedding_dim, dropout, max_len)\n",
    "\n",
    "        sa_module = SelfAttention(\n",
    "            self.embedding_dim, \n",
    "            causal=True, \n",
    "            heads=n_head,\n",
    "            dim_head=dim_head,\n",
    "            local_heads=local_heads,\n",
    "            local_window_size=local_window_size\n",
    "        )\n",
    "        self.encoder_layer = Block(\n",
    "            self.embedding_dim, \n",
    "            dim_feedforward, \n",
    "            dropout, \n",
    "            sa_module\n",
    "        )\n",
    "        self.transformer_encoder = Encoder(self.encoder_layer, num_layers)\n",
    "        \n",
    "        self.heads = nn.ModuleDict({\n",
    "            key: Head(\n",
    "                self.embedding_dim, \n",
    "                head_hidden, \n",
    "                num_classes\n",
    "            ) for key, num_classes in self.num_classes_dict.items()\n",
    "        })\n",
    "\n",
    "    def forward(self, x: torch.Tensor, device: str=\"cpu\") -> torch.Tensor:\n",
    "        embeddings = self.transaction_encoder(x, device=device)\n",
    "        embeddings = self.pos_emb(embeddings)\n",
    "        \n",
    "        padding_mask = self.generate_padding_mask(x[self.cat_cols[0]]).to(device)\n",
    "        embeddings = self.transformer_encoder(embeddings, input_mask=padding_mask)\n",
    "\n",
    "        logits = {key: self.heads[key](embeddings) for key in self.cat_cols}\n",
    "        return logits\n",
    "    \n",
    "    @staticmethod\n",
    "    def generate_padding_mask(x: torch.Tensor) -> torch.Tensor:\n",
    "        return torch.where(x == 0, True, False).bool()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "unable to import cuda code for auto-regressive Performer. will default to the memory inefficient non-cuda version\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'small_group': tensor([[[-0.3667, -0.0030, -0.3110,  ...,  0.0480, -0.1122,  0.3181],\n",
       "          [-0.2650,  0.1266, -0.3726,  ..., -0.0937,  0.0836,  0.2295],\n",
       "          [-0.4515, -0.0009, -0.2546,  ..., -0.1778, -0.2180,  0.2491],\n",
       "          ...,\n",
       "          [-0.1901,  0.0750,  0.0206,  ...,  0.1454, -0.0791,  0.1635],\n",
       "          [-0.2320,  0.0167, -0.0207,  ...,  0.0827, -0.0769,  0.2498],\n",
       "          [-0.2183, -0.0380, -0.1325,  ..., -0.0200, -0.1241,  0.3033]],\n",
       " \n",
       "         [[-0.0689,  0.0600, -0.1273,  ...,  0.0731, -0.2580,  0.1891],\n",
       "          [-0.0514,  0.0196, -0.2422,  ...,  0.0640, -0.3215,  0.1445],\n",
       "          [-0.0525,  0.0692, -0.2134,  ..., -0.0655, -0.4557,  0.2749],\n",
       "          ...,\n",
       "          [-0.1824,  0.1854, -0.2588,  ...,  0.1450, -0.3170,  0.1680],\n",
       "          [-0.2112,  0.0538, -0.2014,  ...,  0.1751, -0.3032,  0.2836],\n",
       "          [-0.2160,  0.0530, -0.2101,  ..., -0.0067, -0.1741,  0.2699]],\n",
       " \n",
       "         [[ 0.0103,  0.1148, -0.3850,  ...,  0.2742,  0.0169,  0.0447],\n",
       "          [-0.0418,  0.0788, -0.2896,  ...,  0.0625, -0.0627,  0.0949],\n",
       "          [-0.1156,  0.0259, -0.1949,  ...,  0.1517,  0.0051,  0.0180],\n",
       "          ...,\n",
       "          [ 0.0179,  0.0731, -0.3412,  ...,  0.0522, -0.0461,  0.0978],\n",
       "          [-0.0234, -0.0086, -0.2670,  ...,  0.0630, -0.1432,  0.2022],\n",
       "          [-0.0374,  0.1159, -0.3363,  ...,  0.0664, -0.0352,  0.1775]]],\n",
       "        grad_fn=<ViewBackward0>),\n",
       " 'amount_rur_bin': tensor([[[ 0.2061,  0.3219, -0.4237,  ...,  0.1067, -0.0041,  0.1231],\n",
       "          [ 0.0944,  0.2818, -0.2033,  ...,  0.0513, -0.2270, -0.1794],\n",
       "          [ 0.1978,  0.1843, -0.2727,  ...,  0.2990, -0.1147,  0.0273],\n",
       "          ...,\n",
       "          [ 0.1055, -0.0509, -0.1842,  ...,  0.2454,  0.0622,  0.3714],\n",
       "          [ 0.2158,  0.0167, -0.1994,  ...,  0.2012,  0.0873,  0.2493],\n",
       "          [ 0.2319, -0.0519, -0.3495,  ...,  0.0747,  0.1371,  0.2147]],\n",
       " \n",
       "         [[ 0.0890,  0.2680, -0.2994,  ...,  0.3439,  0.0320,  0.2558],\n",
       "          [-0.0466,  0.1966, -0.2597,  ...,  0.1734,  0.0254,  0.2721],\n",
       "          [ 0.0725,  0.1123, -0.2692,  ...,  0.2567,  0.0619,  0.3688],\n",
       "          ...,\n",
       "          [ 0.1640,  0.0067, -0.2729,  ...,  0.1726,  0.3546,  0.3357],\n",
       "          [ 0.2170,  0.0333, -0.4209,  ...,  0.1088,  0.2590,  0.4000],\n",
       "          [ 0.1851,  0.1511, -0.3544,  ...,  0.0508,  0.3088,  0.1591]],\n",
       " \n",
       "         [[ 0.0542,  0.2611, -0.3455,  ..., -0.0808, -0.1174, -0.0624],\n",
       "          [ 0.0195,  0.4332, -0.0992,  ..., -0.0249,  0.1327, -0.0595],\n",
       "          [-0.0717,  0.3676, -0.0908,  ...,  0.0244,  0.1518,  0.0027],\n",
       "          ...,\n",
       "          [ 0.1968,  0.1425, -0.3090,  ...,  0.1642,  0.0962, -0.0451],\n",
       "          [ 0.2841,  0.0920, -0.3859,  ...,  0.1229,  0.2296, -0.0243],\n",
       "          [ 0.3053,  0.0826, -0.3123,  ...,  0.1036,  0.0302, -0.0329]]],\n",
       "        grad_fn=<ViewBackward0>)}"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = PerformerModel(\n",
    "    feature_embeddings={\"small_group\": (345, 64), \"amount_rur_bin\": (11, 64)}, \n",
    "    linear_proj=64,\n",
    ")\n",
    "\n",
    "batch = transaction_collate_fn([train_ds[0], train_ds[1], train_ds[2]])\n",
    "model(batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "class TransactionEncoder(nn.Module):\n",
    "    def __init__(self, feature_embeddings: dict[str, tuple[int, int]], linear_proj: int=None):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.feature_embeddings = feature_embeddings\n",
    "        self.embeddings = nn.ModuleDict({key: nn.Embedding(vocab, dim) for key, (vocab, dim) in feature_embeddings.items()})\n",
    "        \n",
    "        if linear_proj is not None:\n",
    "            self.embedding_dim = linear_proj\n",
    "            self.linear_proj = nn.Linear(sum([dim for key, (vocab, dim) in feature_embeddings.items()]), linear_proj)\n",
    "        else:\n",
    "            self.embedding_dim = sum([dim for key, (vocab, dim) in feature_embeddings.items()])\n",
    "            self.linear_proj = nn.Identity()\n",
    "\n",
    "    def forward(self, x: torch.Tensor, device: str=\"cpu\") -> torch.Tensor:\n",
    "        embeddings = [self.embeddings[key](x[key].to(device)) for key in self.feature_embeddings]\n",
    "        proj = self.linear_proj(torch.cat(embeddings, dim=2))\n",
    "        return proj\n",
    "\n",
    "\n",
    "class Head(nn.Module):\n",
    "    def __init__(self, embedding_dim: int, hidden_dim: int, vocab_size: int):\n",
    "        super().__init__()\n",
    "\n",
    "        self.head = nn.Sequential(\n",
    "            nn.Linear(embedding_dim, hidden_dim),\n",
    "            nn.GELU(),\n",
    "            nn.Linear(hidden_dim, vocab_size),\n",
    "        )\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        return self.head(x)\n",
    "\n",
    "\n",
    "class PositionalEncoding(nn.Module):\n",
    "    def __init__(self, embedding_dim: int, dropout: float=0.1, max_len: int=1000):\n",
    "        super().__init__()\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "        position = torch.arange(max_len).unsqueeze(1)\n",
    "        div_term = torch.exp(torch.arange(0, embedding_dim, 2) * (-math.log(10000.0) / embedding_dim))\n",
    "        pe = torch.zeros(1, max_len, embedding_dim)\n",
    "        pe[0, :, 0::2] = torch.sin(position * div_term)\n",
    "        pe[0, :, 1::2] = torch.cos(position * div_term)\n",
    "        self.register_buffer('pe', pe)\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Arguments:\n",
    "            x: Tensor, shape ``[batch_size, seq_len, embedding_dim]``\n",
    "        \"\"\"\n",
    "        x = x + self.pe[:, :x.size(1), :]\n",
    "        return self.dropout(x)\n",
    "\n",
    "\n",
    "class PerformerModel(nn.Module):\n",
    "    def __init__(\n",
    "            self, \n",
    "            feature_embeddings: dict[str, tuple[int, int]], \n",
    "            linear_proj: int=None,\n",
    "            n_head: int=8, \n",
    "            dim_feedforward: int=128, \n",
    "            dropout: float=0.1, \n",
    "            num_layers: int=6, \n",
    "            head_hidden: int=128,\n",
    "            max_len: int=1000,\n",
    "        ):\n",
    "        super().__init__()\n",
    "\n",
    "        self.transaction_encoder = TransactionEncoder(feature_embeddings, linear_proj=linear_proj)\n",
    "        self.embedding_dim = self.transaction_encoder.embedding_dim\n",
    "        self.cat_cols = list(feature_embeddings.keys())\n",
    "        self.num_classes_dict = {key: num_classes for key, (num_classes, _) in feature_embeddings.items()}\n",
    "        \n",
    "        self.pos_emb = PositionalEncoding(self.embedding_dim, dropout, max_len)\n",
    "\n",
    "        self.transformer_encoder = Performer(\n",
    "            dim = self.embedding_dim, \n",
    "            depth = num_layers,\n",
    "            heads = n_head, \n",
    "            ff_dropout = dropout,\n",
    "            causal = True,\n",
    "            dim_head = 16,\n",
    "            # use_rezero = True,\n",
    "            # no_projection = True,\n",
    "            # feature_redraw_interval = 10000000,\n",
    "            # bucket_size = 25\n",
    "        )\n",
    "        \n",
    "        self.heads = nn.ModuleDict({\n",
    "            key: Head(\n",
    "                self.embedding_dim, \n",
    "                head_hidden, \n",
    "                num_classes\n",
    "            ) for key, num_classes in self.num_classes_dict.items()\n",
    "        })\n",
    "\n",
    "    def forward(self, x: torch.Tensor, device: str=\"cpu\") -> torch.Tensor:\n",
    "        N, S = x[self.cat_cols[0]].shape\n",
    "        embeddings = self.transaction_encoder(x, device=device)\n",
    "        embeddings = self.pos_emb(embeddings)\n",
    "        \n",
    "        attn_mask = self.generate_square_subsequent_mask(S).to(device)\n",
    "        padding_mask = self.generate_padding_mask(x[self.cat_cols[0]]).to(device)\n",
    "        embeddings = self.transformer_encoder(embeddings, input_mask=padding_mask)\n",
    "\n",
    "        logits = {key: self.heads[key](embeddings) for key in self.cat_cols}\n",
    "        return logits\n",
    "\n",
    "    @staticmethod\n",
    "    def generate_square_subsequent_mask(sz: int) -> torch.Tensor:\n",
    "        return torch.triu(torch.full((sz, sz), True), diagonal=1).bool()\n",
    "    \n",
    "    @staticmethod\n",
    "    def generate_padding_mask(x: torch.Tensor) -> torch.Tensor:\n",
    "        return torch.where(x == 0, True, False).bool()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm.notebook import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "def train_epoch(model, optimizer, dataloader, warmup=10, device=\"cuda\"):\n",
    "    model.train()\n",
    "    model.to(device)\n",
    "\n",
    "    metrics = {\n",
    "        key: {\n",
    "            \"f1_score\": MulticlassF1Score(\n",
    "                num_classes=num_classes, \n",
    "                average=\"weighted\", \n",
    "                ignore_index=0\n",
    "            ), \n",
    "            \"accuracy\": Accuracy(\n",
    "                task=\"multiclass\", \n",
    "                num_classes=num_classes, \n",
    "                ignore_index=0\n",
    "            )\n",
    "        } for key, num_classes in model.num_classes_dict.items()\n",
    "    }\n",
    "\n",
    "    loss_epoch = 0\n",
    "    count = 0 \n",
    "    for batch_dict in tqdm(dataloader):\n",
    "        logits_dict = model(batch_dict, device=device)\n",
    "\n",
    "        loss = 0\n",
    "        for key, logits in logits_dict.items():\n",
    "            y = batch_dict[key][:, warmup + 1:].to(device)\n",
    "            logits_pred = logits[:, warmup: -1].permute(0, 2, 1)    # B x C x T\n",
    "\n",
    "            loss += nn.functional.cross_entropy(logits_pred, y, ignore_index=0)\n",
    "            \n",
    "            y_pred = logits_pred.argmax(dim=1).to(\"cpu\")\n",
    "            metrics[key][\"f1_score\"].update(y_pred, y.to(\"cpu\"))\n",
    "            metrics[key][\"accuracy\"].update(y_pred, y.to(\"cpu\"))\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        cur_count =  torch.sum((y != 0).float()).item()\n",
    "        loss_epoch += loss.item() * cur_count\n",
    "        count += cur_count\n",
    "\n",
    "    return loss_epoch / count, {feature: {m: v.compute().item() for m, v in results.items()} for feature, results in metrics.items()}\n",
    "\n",
    "\n",
    "def eval_epoch(model, dataloader, warmup=10, device=\"cuda\"):\n",
    "    model.eval()\n",
    "    model.to(device)\n",
    "\n",
    "    metrics = {\n",
    "        key: {\n",
    "            \"f1_score\": MulticlassF1Score(\n",
    "                num_classes=num_classes, \n",
    "                average=\"weighted\", \n",
    "                ignore_index=0\n",
    "            ), \n",
    "            \"accuracy\": Accuracy(\n",
    "                task=\"multiclass\", \n",
    "                num_classes=num_classes, \n",
    "                ignore_index=0\n",
    "            )\n",
    "        } for key, num_classes in model.num_classes_dict.items()\n",
    "    }\n",
    "\n",
    "    with torch.no_grad():\n",
    "        loss_epoch = 0\n",
    "        count = 0 \n",
    "        for batch_dict in dataloader:\n",
    "            logits_dict = model(batch_dict, device=device)\n",
    "\n",
    "            loss = 0\n",
    "            for key, logits in logits_dict.items():\n",
    "                y = batch_dict[key][:, warmup + 1:].to(device)\n",
    "                logits_pred = logits[:, warmup: -1].permute(0, 2, 1)\n",
    "        \n",
    "                loss += nn.functional.cross_entropy(logits_pred, y, ignore_index=0)\n",
    "\n",
    "                y_pred = logits_pred.argmax(dim=1).to(\"cpu\")\n",
    "                metrics[key][\"f1_score\"].update(y_pred, y.to(\"cpu\"))\n",
    "                metrics[key][\"accuracy\"].update(y_pred, y.to(\"cpu\"))\n",
    "\n",
    "            cur_count = torch.sum((y != 0).float()).item()\n",
    "            loss_epoch += loss.item() * cur_count\n",
    "            count += cur_count    \n",
    "\n",
    "    return loss_epoch / count, {feature: {m: v.compute().item() for m, v in results.items()} for feature, results in metrics.items()}\n",
    "\n",
    "\n",
    "def train_model(model, optimizer, dataloaders, n_epochs, warmup=10, device=\"cuda\", save_path=\"./\"):\n",
    "    if not os.path.exists(save_path):\n",
    "        os.mkdir(save_path)\n",
    "\n",
    "    best_loss = float(\"inf\")\n",
    "    for epoch in tqdm(range(n_epochs)):\n",
    "        train_start = time.perf_counter()\n",
    "        train_loss, train_metrics = train_epoch(model, optimizer, dataloaders[\"train\"], warmup, device)\n",
    "        train_end = time.perf_counter()\n",
    "        val_loss, val_metrics = eval_epoch(model, dataloaders[\"val\"], warmup, device)\n",
    "        val_end = time.perf_counter()\n",
    "\n",
    "        # wandb.log({\n",
    "        #     \"Epoch\": epoch+1,\n",
    "        #     \"Train time\": train_end - train_start,\n",
    "        #     \"Train loss\": train_loss,\n",
    "        #     \"Train metrics\": train_metrics,\n",
    "        #     \"Val time\": val_end - train_end,\n",
    "        #     \"Val metrics\": val_metrics,\n",
    "        #     \"Val loss\": val_loss\n",
    "        # })\n",
    "\n",
    "        if val_loss < best_loss:\n",
    "             best_loss = val_loss\n",
    "             torch.save(model, os.path.join(save_path, \"best_model.pt\"))\n",
    "\n",
    "    model = torch.load(os.path.join(save_path, \"best_model.pt\"))\n",
    "\n",
    "    test_start = time.perf_counter()\n",
    "    test_loss, test_metrics = eval_epoch(model, dataloaders[\"test\"], warmup, device)\n",
    "    test_end = time.perf_counter()\n",
    "\n",
    "    # wandb.summary[\"Test time\"] = test_end - test_start\n",
    "    # wandb.summary[\"Test metrics\"] = test_metrics\n",
    "    # wandb.summary[\"Test loss\"] = test_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = DataLoader(train_ds, batch_size=32, shuffle=True, collate_fn=transaction_collate_fn)\n",
    "val_loader = DataLoader(val_ds, batch_size=32, shuffle=False, collate_fn=transaction_collate_fn)\n",
    "test_loader = DataLoader(test_ds, batch_size=32, shuffle=False, collate_fn=transaction_collate_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "unable to import cuda code for auto-regressive Performer. will default to the memory inefficient non-cuda version\n",
      "unable to import cuda code for auto-regressive Performer. will default to the memory inefficient non-cuda version\n",
      "unable to import cuda code for auto-regressive Performer. will default to the memory inefficient non-cuda version\n",
      "unable to import cuda code for auto-regressive Performer. will default to the memory inefficient non-cuda version\n",
      "unable to import cuda code for auto-regressive Performer. will default to the memory inefficient non-cuda version\n",
      "unable to import cuda code for auto-regressive Performer. will default to the memory inefficient non-cuda version\n"
     ]
    }
   ],
   "source": [
    "transformer = PerformerModel(\n",
    "    feature_embeddings={\"small_group\": (345, 64), \"amount_rur_bin\": (11, 64)}, \n",
    "    linear_proj=64,\n",
    "    n_head=8, \n",
    "    dim_feedforward=128, \n",
    "    dropout=0.1, \n",
    "    num_layers=6, \n",
    "    head_hidden=128\n",
    ")\n",
    "\n",
    "optimizer = torch.optim.Adam(transformer.parameters(), lr=1e-3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'fast_transformers.causal_product.causal_product_cuda'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[17], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mfast_transformers\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mcausal_product\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mcausal_product_cuda\u001b[39;00m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'fast_transformers.causal_product.causal_product_cuda'"
     ]
    }
   ],
   "source": [
    "fast"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5693d94b9e894d3a9943bc0e44f6b355",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/20 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "39c1505242db40b2b7267d70e0b66e5f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/106 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fe5c8494b72544b8b9a267a485275b81",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/106 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[16], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m train_model(\n\u001b[1;32m      2\u001b[0m     transformer, \n\u001b[1;32m      3\u001b[0m     optimizer,\n\u001b[1;32m      4\u001b[0m     {\u001b[39m\"\u001b[39m\u001b[39mtrain\u001b[39m\u001b[39m\"\u001b[39m: train_loader, \u001b[39m\"\u001b[39m\u001b[39mval\u001b[39m\u001b[39m\"\u001b[39m: val_loader, \u001b[39m\"\u001b[39m\u001b[39mtest\u001b[39m\u001b[39m\"\u001b[39m: test_loader},\n\u001b[1;32m      5\u001b[0m     \u001b[39m20\u001b[39m\n\u001b[1;32m      6\u001b[0m )\n",
      "Cell \u001b[0;32mIn[13], line 99\u001b[0m, in \u001b[0;36mtrain_model\u001b[0;34m(model, optimizer, dataloaders, n_epochs, warmup, device, save_path)\u001b[0m\n\u001b[1;32m     97\u001b[0m \u001b[39mfor\u001b[39;00m epoch \u001b[39min\u001b[39;00m tqdm(\u001b[39mrange\u001b[39m(n_epochs)):\n\u001b[1;32m     98\u001b[0m     train_start \u001b[39m=\u001b[39m time\u001b[39m.\u001b[39mperf_counter()\n\u001b[0;32m---> 99\u001b[0m     train_loss, train_metrics \u001b[39m=\u001b[39m train_epoch(model, optimizer, dataloaders[\u001b[39m\"\u001b[39m\u001b[39mtrain\u001b[39m\u001b[39m\"\u001b[39m], warmup, device)\n\u001b[1;32m    100\u001b[0m     train_end \u001b[39m=\u001b[39m time\u001b[39m.\u001b[39mperf_counter()\n\u001b[1;32m    101\u001b[0m     val_loss, val_metrics \u001b[39m=\u001b[39m eval_epoch(model, dataloaders[\u001b[39m\"\u001b[39m\u001b[39mval\u001b[39m\u001b[39m\"\u001b[39m], warmup, device)\n",
      "Cell \u001b[0;32mIn[13], line 39\u001b[0m, in \u001b[0;36mtrain_epoch\u001b[0;34m(model, optimizer, dataloader, warmup, device)\u001b[0m\n\u001b[1;32m     36\u001b[0m     metrics[key][\u001b[39m\"\u001b[39m\u001b[39maccuracy\u001b[39m\u001b[39m\"\u001b[39m]\u001b[39m.\u001b[39mupdate(y_pred, y\u001b[39m.\u001b[39mto(\u001b[39m\"\u001b[39m\u001b[39mcpu\u001b[39m\u001b[39m\"\u001b[39m))\n\u001b[1;32m     38\u001b[0m optimizer\u001b[39m.\u001b[39mzero_grad()\n\u001b[0;32m---> 39\u001b[0m loss\u001b[39m.\u001b[39mbackward()\n\u001b[1;32m     40\u001b[0m optimizer\u001b[39m.\u001b[39mstep()\n\u001b[1;32m     42\u001b[0m cur_count \u001b[39m=\u001b[39m  torch\u001b[39m.\u001b[39msum((y \u001b[39m!=\u001b[39m \u001b[39m0\u001b[39m)\u001b[39m.\u001b[39mfloat())\u001b[39m.\u001b[39mitem()\n",
      "File \u001b[0;32m~/personal/anaconda3/envs/main4/lib/python3.11/site-packages/torch/_tensor.py:487\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    477\u001b[0m \u001b[39mif\u001b[39;00m has_torch_function_unary(\u001b[39mself\u001b[39m):\n\u001b[1;32m    478\u001b[0m     \u001b[39mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    479\u001b[0m         Tensor\u001b[39m.\u001b[39mbackward,\n\u001b[1;32m    480\u001b[0m         (\u001b[39mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    485\u001b[0m         inputs\u001b[39m=\u001b[39minputs,\n\u001b[1;32m    486\u001b[0m     )\n\u001b[0;32m--> 487\u001b[0m torch\u001b[39m.\u001b[39mautograd\u001b[39m.\u001b[39mbackward(\n\u001b[1;32m    488\u001b[0m     \u001b[39mself\u001b[39m, gradient, retain_graph, create_graph, inputs\u001b[39m=\u001b[39minputs\n\u001b[1;32m    489\u001b[0m )\n",
      "File \u001b[0;32m~/personal/anaconda3/envs/main4/lib/python3.11/site-packages/torch/autograd/__init__.py:200\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    195\u001b[0m     retain_graph \u001b[39m=\u001b[39m create_graph\n\u001b[1;32m    197\u001b[0m \u001b[39m# The reason we repeat same the comment below is that\u001b[39;00m\n\u001b[1;32m    198\u001b[0m \u001b[39m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    199\u001b[0m \u001b[39m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 200\u001b[0m Variable\u001b[39m.\u001b[39m_execution_engine\u001b[39m.\u001b[39mrun_backward(  \u001b[39m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[1;32m    201\u001b[0m     tensors, grad_tensors_, retain_graph, create_graph, inputs,\n\u001b[1;32m    202\u001b[0m     allow_unreachable\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m, accumulate_grad\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "train_model(\n",
    "    transformer, \n",
    "    optimizer,\n",
    "    {\"train\": train_loader, \"val\": val_loader, \"test\": test_loader},\n",
    "    20\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = df.sort_values(\"TRDATETIME\").groupby(\"client_id\").apply(lambda x: x[\"amount_rur_bin\"] == x[\"amount_rur_bin\"].shift())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.17164274952957415"
      ]
     },
     "execution_count": 111,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a.reset_index()[\"amount_rur_bin\"].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = {\n",
    "    \"experiment_name\": \"run2\",\n",
    "    \"dataset\": \"rosbank\",\n",
    "    \"min_length\": 20,\n",
    "    \"max_length\": 100,\n",
    "    \"batch_size\": 32,\n",
    "    \"type\": \"transformer\",\n",
    "    \"transformer_params\": {\n",
    "        \"feature_embeddings\": {\"small_group\": (345, 64), \"amount_rur_bin\": (11, 64)}, \n",
    "        \"linear_proj\": 64,\n",
    "        \"n_head\": 8, \n",
    "        \"dim_feedforward\": 128, \n",
    "        \"dropout\": 0.1, \n",
    "        \"num_layers\": 6, \n",
    "        \"head_hidden\": 128,\n",
    "    },\n",
    "    \"lr\": 1e-3,\n",
    "    \"n_epochs\": 50,\n",
    "    \"warmup\": 10,\n",
    "    \"device\": \"cuda\",\n",
    "    \"save_path\": \"./\"\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "with open(\"config.json\", \"w\") as f:\n",
    "    json.dump(config, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "from yaml import load, dump\n",
    "# from yaml import CLoader as Loader, CDumper as Dumper\n",
    "\n",
    "with open(\"config.yaml\", \"w\") as f:\n",
    "    dump(config, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'././'"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.path.exists(os.path.join(\"./\", \"./\"))\n",
    "os.path.join(\"./\", \"./\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "load() missing 1 required positional argument: 'Loader'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[59], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[39mwith\u001b[39;00m \u001b[39mopen\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39mconfig.yaml\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mr\u001b[39m\u001b[39m\"\u001b[39m) \u001b[39mas\u001b[39;00m f:\n\u001b[1;32m----> 2\u001b[0m     d \u001b[39m=\u001b[39m load(f)\n\u001b[0;32m      4\u001b[0m d\n",
      "\u001b[1;31mTypeError\u001b[0m: load() missing 1 required positional argument: 'Loader'"
     ]
    }
   ],
   "source": [
    "\n",
    "with open(\"config.yaml\", \"r\") as f:\n",
    "    d = load(f)\n",
    "\n",
    "d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main(config):\n",
    "    # wandb.login()\n",
    "\n",
    "    # wandb.init(\n",
    "    #     project=\"deep-learning-project\",\n",
    "    #     name=config[\"experiment_name\"], \n",
    "    #     tags=config[\"tags\"],\n",
    "    #     config=config\n",
    "    # )\n",
    "\n",
    "    if config[\"dataset\"] == \"rosbank\":\n",
    "        df = pd.read_csv('data/rosbank/train.csv')\n",
    "        df['TRDATETIME'] = pd.to_datetime(df['TRDATETIME'], format='%d%b%y:%H:%M:%S')\n",
    "        df = df.rename(columns={'cl_id':'client_id', 'MCC':'small_group', 'amount':'amount_rur'})\n",
    "        \n",
    "        mcc_to_id = {mcc: i+1 for i, mcc in enumerate(df['small_group'].unique())}\n",
    "\n",
    "        df['amount_rur_bin'] = 1 + KBinsDiscretizer(10, encode='ordinal', subsample=None).fit_transform(df[['amount_rur']]).astype('int')\n",
    "        df['small_group'] = df['small_group'].map(mcc_to_id)\n",
    "\n",
    "    else:\n",
    "        pass\n",
    "\n",
    "    clients_train, clients_val_test = train_test_split(df[\"client_id\"].unique(), test_size=0.2, random_state=42)\n",
    "    clients_val, clients_test = train_test_split(clients_val_test, test_size=0.5, random_state=42)\n",
    "\n",
    "    train_ds = TransactionDataset(\n",
    "        df[lambda x: x[\"client_id\"].isin(clients_train)], \n",
    "        id_col=\"client_id\", \n",
    "        dt_col=\"TRDATETIME\", \n",
    "        cat_cols=[\"small_group\", \"amount_rur_bin\"],\n",
    "        min_length=config[\"min_length\"],\n",
    "        max_length=config[\"max_length\"],\n",
    "        random_slice=True\n",
    "    )\n",
    "\n",
    "    val_ds = TransactionDataset(\n",
    "        df[lambda x: x[\"client_id\"].isin(clients_val)], \n",
    "        id_col=\"client_id\", \n",
    "        dt_col=\"TRDATETIME\", \n",
    "        cat_cols=[\"small_group\", \"amount_rur_bin\"],\n",
    "        min_length=config[\"min_length\"],\n",
    "        max_length=config[\"max_length\"],\n",
    "        random_slice=False\n",
    "    )\n",
    "\n",
    "    test_ds = TransactionDataset(\n",
    "        df[lambda x: x[\"client_id\"].isin(clients_test)], \n",
    "        id_col=\"client_id\", \n",
    "        dt_col=\"TRDATETIME\", \n",
    "        cat_cols=[\"small_group\", \"amount_rur_bin\"],\n",
    "        min_length=config[\"min_length\"],\n",
    "        max_length=config[\"max_length\"],\n",
    "        random_slice=False\n",
    "    )\n",
    "\n",
    "    train_loader = DataLoader(train_ds, batch_size=config[\"batch_size\"], shuffle=True, collate_fn=transaction_collate_fn)\n",
    "    val_loader = DataLoader(val_ds, batch_size=config[\"batch_size\"], shuffle=False, collate_fn=transaction_collate_fn)\n",
    "    test_loader = DataLoader(test_ds, batch_size=config[\"batch_size\"], shuffle=False, collate_fn=transaction_collate_fn)\n",
    "\n",
    "    transformer = TransformerModel(**config[\"transformer_params\"])\n",
    "    optimizer = torch.optim.Adam(transformer.parameters(), lr=config[\"lr\"])\n",
    "\n",
    "    train_model(\n",
    "        transformer, \n",
    "        optimizer, \n",
    "        {\"train\": train_loader, \"val\": val_loader, \"test\": test_loader}, \n",
    "        n_epochs=config[\"n_epochs\"],\n",
    "        warmup=config[\"warmup\"],\n",
    "        device=config[\"device\"],\n",
    "        save_path=config[\"save_path\"]\n",
    "    )\n",
    "\n",
    "    # wandb.finish()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Calling wandb.login() after wandb.init() has no effect.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Finishing last run (ID:bu3ggt4d) before initializing another..."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a3d4f01021c4472eafce6656a6a5a0b8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.001 MB of 0.001 MB uploaded (0.000 MB deduped)\\r'), FloatProgress(value=1.0, max…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">run2</strong> at: <a href='https://wandb.ai/aayugay99/deep-learning-project/runs/bu3ggt4d' target=\"_blank\">https://wandb.ai/aayugay99/deep-learning-project/runs/bu3ggt4d</a><br/>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>.\\wandb\\run-20230522_184446-bu3ggt4d\\logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Successfully finished last run (ID:bu3ggt4d). Initializing new run:<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "69ac9dc73049497f8e49d12a39acf682",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='Waiting for wandb.init()...\\r'), FloatProgress(value=0.016666666666666666, max=1.0…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.15.3 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.14.0"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>d:\\Skoltech\\Deep Learning\\dl2023\\efficient_transformers_on_transactional_data\\wandb\\run-20230522_184723-cgbc8t2b</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/aayugay99/deep-learning-project/runs/cgbc8t2b' target=\"_blank\">run2</a></strong> to <a href='https://wandb.ai/aayugay99/deep-learning-project' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/aayugay99/deep-learning-project' target=\"_blank\">https://wandb.ai/aayugay99/deep-learning-project</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/aayugay99/deep-learning-project/runs/cgbc8t2b' target=\"_blank\">https://wandb.ai/aayugay99/deep-learning-project/runs/cgbc8t2b</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "628 sequences were filtered\n",
      "77 sequences were filtered\n",
      "70 sequences were filtered\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\User\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\torch\\nn\\modules\\transformer.py:562: UserWarning: Converting mask without torch.bool dtype to bool; this will negatively affect performance. Prefer to use a boolean mask directly. (Triggered internally at C:\\actions-runner\\_work\\pytorch\\pytorch\\builder\\windows\\pytorch\\aten\\src\\ATen\\native\\transformers\\attention.cpp:152.)\n",
      "  return torch._transformer_encoder_layer_fwd(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>Epoch</td><td>▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇███</td></tr><tr><td>Train loss</td><td>█▅▄▃▃▃▃▃▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>Train time</td><td>█▅▂▁▁▁▂▁▁▁▁▁▂▁▁▂▁▂▁▁▁▁▂▁▂▁▁▁▁▂▂▁▂▂▂▂▁▁▁▁</td></tr><tr><td>Val loss</td><td>█▅▄▄▃▂▂▂▂▁▁▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▂▂▂▁▂</td></tr><tr><td>Val time</td><td>▂▃▁▃▂▁▁▁▁▁▃▃▂▁▁▂▁▁▁▁▁▁▁█▁▁▂▁▂█▂▃▂▃▂▆▁▁▃▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>Epoch</td><td>50</td></tr><tr><td>Test loss</td><td>4.94315</td></tr><tr><td>Test time</td><td>0.19568</td></tr><tr><td>Train loss</td><td>4.75581</td></tr><tr><td>Train time</td><td>3.7141</td></tr><tr><td>Val loss</td><td>5.037</td></tr><tr><td>Val time</td><td>0.19415</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">run2</strong> at: <a href='https://wandb.ai/aayugay99/deep-learning-project/runs/cgbc8t2b' target=\"_blank\">https://wandb.ai/aayugay99/deep-learning-project/runs/cgbc8t2b</a><br/>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>.\\wandb\\run-20230522_184723-cgbc8t2b\\logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "main(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wandb.login()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "cf92aa13fedf815d5c8dd192b8d835913fde3e8bc926b2a0ad6cc74ef2ba3ca2"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
