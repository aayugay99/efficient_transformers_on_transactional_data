{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import KBinsDiscretizer\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn \n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "\n",
    "from dataset import TransactionDataset, transaction_collate_fn\n",
    "\n",
    "from torchmetrics.classification import MulticlassF1Score, Accuracy\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import time\n",
    "\n",
    "import wandb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>PERIOD</th>\n",
       "      <th>client_id</th>\n",
       "      <th>small_group</th>\n",
       "      <th>channel_type</th>\n",
       "      <th>currency</th>\n",
       "      <th>TRDATETIME</th>\n",
       "      <th>amount_rur</th>\n",
       "      <th>trx_category</th>\n",
       "      <th>target_flag</th>\n",
       "      <th>target_sum</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>01/10/2017</td>\n",
       "      <td>0</td>\n",
       "      <td>5200</td>\n",
       "      <td>NaN</td>\n",
       "      <td>810</td>\n",
       "      <td>2017-10-21 00:00:00</td>\n",
       "      <td>5023.0</td>\n",
       "      <td>POS</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>01/10/2017</td>\n",
       "      <td>0</td>\n",
       "      <td>6011</td>\n",
       "      <td>NaN</td>\n",
       "      <td>810</td>\n",
       "      <td>2017-10-12 12:24:07</td>\n",
       "      <td>20000.0</td>\n",
       "      <td>DEPOSIT</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>01/12/2017</td>\n",
       "      <td>0</td>\n",
       "      <td>5921</td>\n",
       "      <td>NaN</td>\n",
       "      <td>810</td>\n",
       "      <td>2017-12-05 00:00:00</td>\n",
       "      <td>767.0</td>\n",
       "      <td>POS</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>01/10/2017</td>\n",
       "      <td>0</td>\n",
       "      <td>5411</td>\n",
       "      <td>NaN</td>\n",
       "      <td>810</td>\n",
       "      <td>2017-10-21 00:00:00</td>\n",
       "      <td>2031.0</td>\n",
       "      <td>POS</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>01/10/2017</td>\n",
       "      <td>0</td>\n",
       "      <td>6012</td>\n",
       "      <td>NaN</td>\n",
       "      <td>810</td>\n",
       "      <td>2017-10-24 13:14:24</td>\n",
       "      <td>36562.0</td>\n",
       "      <td>C2C_OUT</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       PERIOD  client_id  small_group channel_type  currency  \\\n",
       "0  01/10/2017          0         5200          NaN       810   \n",
       "1  01/10/2017          0         6011          NaN       810   \n",
       "2  01/12/2017          0         5921          NaN       810   \n",
       "3  01/10/2017          0         5411          NaN       810   \n",
       "4  01/10/2017          0         6012          NaN       810   \n",
       "\n",
       "           TRDATETIME  amount_rur trx_category  target_flag  target_sum  \n",
       "0 2017-10-21 00:00:00      5023.0          POS            0         0.0  \n",
       "1 2017-10-12 12:24:07     20000.0      DEPOSIT            0         0.0  \n",
       "2 2017-12-05 00:00:00       767.0          POS            0         0.0  \n",
       "3 2017-10-21 00:00:00      2031.0          POS            0         0.0  \n",
       "4 2017-10-24 13:14:24     36562.0      C2C_OUT            0         0.0  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('data/rosbank/train.csv')\n",
    "df['TRDATETIME'] = pd.to_datetime(df['TRDATETIME'], format='%d%b%y:%H:%M:%S')\n",
    "df = df.rename(columns={'cl_id':'client_id', 'MCC':'small_group', 'amount':'amount_rur'})\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>client_id</th>\n",
       "      <th>trans_date</th>\n",
       "      <th>small_group</th>\n",
       "      <th>amount_rur</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>33172</td>\n",
       "      <td>6</td>\n",
       "      <td>4</td>\n",
       "      <td>71.463</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>33172</td>\n",
       "      <td>6</td>\n",
       "      <td>35</td>\n",
       "      <td>45.017</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>33172</td>\n",
       "      <td>8</td>\n",
       "      <td>11</td>\n",
       "      <td>13.887</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>33172</td>\n",
       "      <td>9</td>\n",
       "      <td>11</td>\n",
       "      <td>15.983</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>33172</td>\n",
       "      <td>10</td>\n",
       "      <td>11</td>\n",
       "      <td>21.341</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   client_id  trans_date  small_group  amount_rur\n",
       "0      33172           6            4      71.463\n",
       "1      33172           6           35      45.017\n",
       "2      33172           8           11      13.887\n",
       "3      33172           9           11      15.983\n",
       "4      33172          10           11      21.341"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# df = pd.read_csv('data/sberbank/transactions_train.csv')\n",
    "# df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "mcc_to_id = {mcc: i+1 for i, mcc in enumerate(df['small_group'].unique())}\n",
    "\n",
    "df['amount_rur_bin'] = 1 + KBinsDiscretizer(10, encode='ordinal', subsample=None).fit_transform(df[['amount_rur']]).astype('int')\n",
    "df['small_group'] = df['small_group'].map(mcc_to_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>PERIOD</th>\n",
       "      <th>client_id</th>\n",
       "      <th>small_group</th>\n",
       "      <th>channel_type</th>\n",
       "      <th>currency</th>\n",
       "      <th>TRDATETIME</th>\n",
       "      <th>amount_rur</th>\n",
       "      <th>trx_category</th>\n",
       "      <th>target_flag</th>\n",
       "      <th>target_sum</th>\n",
       "      <th>amount_rur_bin</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>01/10/2017</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>810</td>\n",
       "      <td>2017-10-21 00:00:00</td>\n",
       "      <td>5023.00</td>\n",
       "      <td>POS</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>01/10/2017</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>NaN</td>\n",
       "      <td>810</td>\n",
       "      <td>2017-10-12 12:24:07</td>\n",
       "      <td>20000.00</td>\n",
       "      <td>DEPOSIT</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>01/12/2017</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>NaN</td>\n",
       "      <td>810</td>\n",
       "      <td>2017-12-05 00:00:00</td>\n",
       "      <td>767.00</td>\n",
       "      <td>POS</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>01/10/2017</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>NaN</td>\n",
       "      <td>810</td>\n",
       "      <td>2017-10-21 00:00:00</td>\n",
       "      <td>2031.00</td>\n",
       "      <td>POS</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>01/10/2017</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>NaN</td>\n",
       "      <td>810</td>\n",
       "      <td>2017-10-24 13:14:24</td>\n",
       "      <td>36562.00</td>\n",
       "      <td>C2C_OUT</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>490508</th>\n",
       "      <td>01/04/2017</td>\n",
       "      <td>10176</td>\n",
       "      <td>2</td>\n",
       "      <td>type1</td>\n",
       "      <td>810</td>\n",
       "      <td>2017-04-24 14:05:26</td>\n",
       "      <td>600.00</td>\n",
       "      <td>WD_ATM_ROS</td>\n",
       "      <td>1</td>\n",
       "      <td>405.0</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>490509</th>\n",
       "      <td>01/06/2017</td>\n",
       "      <td>10171</td>\n",
       "      <td>4</td>\n",
       "      <td>type1</td>\n",
       "      <td>810</td>\n",
       "      <td>2017-06-06 00:00:00</td>\n",
       "      <td>132.00</td>\n",
       "      <td>POS</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>490510</th>\n",
       "      <td>01/02/2017</td>\n",
       "      <td>10167</td>\n",
       "      <td>51</td>\n",
       "      <td>type1</td>\n",
       "      <td>810</td>\n",
       "      <td>2017-02-03 00:00:00</td>\n",
       "      <td>1000.00</td>\n",
       "      <td>POS</td>\n",
       "      <td>1</td>\n",
       "      <td>280428.2</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>490511</th>\n",
       "      <td>01/06/2017</td>\n",
       "      <td>10163</td>\n",
       "      <td>39</td>\n",
       "      <td>type1</td>\n",
       "      <td>810</td>\n",
       "      <td>2017-06-08 00:00:00</td>\n",
       "      <td>100.00</td>\n",
       "      <td>POS</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>490512</th>\n",
       "      <td>01/06/2017</td>\n",
       "      <td>10162</td>\n",
       "      <td>4</td>\n",
       "      <td>type1</td>\n",
       "      <td>810</td>\n",
       "      <td>2017-06-15 00:00:00</td>\n",
       "      <td>441.33</td>\n",
       "      <td>POS</td>\n",
       "      <td>1</td>\n",
       "      <td>253.0</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>490513 rows × 11 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            PERIOD  client_id  small_group channel_type  currency  \\\n",
       "0       01/10/2017          0            1          NaN       810   \n",
       "1       01/10/2017          0            2          NaN       810   \n",
       "2       01/12/2017          0            3          NaN       810   \n",
       "3       01/10/2017          0            4          NaN       810   \n",
       "4       01/10/2017          0            5          NaN       810   \n",
       "...            ...        ...          ...          ...       ...   \n",
       "490508  01/04/2017      10176            2        type1       810   \n",
       "490509  01/06/2017      10171            4        type1       810   \n",
       "490510  01/02/2017      10167           51        type1       810   \n",
       "490511  01/06/2017      10163           39        type1       810   \n",
       "490512  01/06/2017      10162            4        type1       810   \n",
       "\n",
       "                TRDATETIME  amount_rur trx_category  target_flag  target_sum  \\\n",
       "0      2017-10-21 00:00:00     5023.00          POS            0         0.0   \n",
       "1      2017-10-12 12:24:07    20000.00      DEPOSIT            0         0.0   \n",
       "2      2017-12-05 00:00:00      767.00          POS            0         0.0   \n",
       "3      2017-10-21 00:00:00     2031.00          POS            0         0.0   \n",
       "4      2017-10-24 13:14:24    36562.00      C2C_OUT            0         0.0   \n",
       "...                    ...         ...          ...          ...         ...   \n",
       "490508 2017-04-24 14:05:26      600.00   WD_ATM_ROS            1       405.0   \n",
       "490509 2017-06-06 00:00:00      132.00          POS            0         0.0   \n",
       "490510 2017-02-03 00:00:00     1000.00          POS            1    280428.2   \n",
       "490511 2017-06-08 00:00:00      100.00          POS            0         0.0   \n",
       "490512 2017-06-15 00:00:00      441.33          POS            1       253.0   \n",
       "\n",
       "        amount_rur_bin  \n",
       "0                    9  \n",
       "1                   10  \n",
       "2                    6  \n",
       "3                    8  \n",
       "4                   10  \n",
       "...                ...  \n",
       "490508               5  \n",
       "490509               2  \n",
       "490510               7  \n",
       "490511               2  \n",
       "490512               5  \n",
       "\n",
       "[490513 rows x 11 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df = df.rename({'trans_date': 'TRDATETIME'}, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "628 sequences were filtered\n",
      "77 sequences were filtered\n",
      "70 sequences were filtered\n"
     ]
    }
   ],
   "source": [
    "clients_train, clients_val_test = train_test_split(df[\"client_id\"].unique(), test_size=0.2, random_state=42)\n",
    "clients_val, clients_test = train_test_split(clients_val_test, test_size=0.5, random_state=42)\n",
    "\n",
    "train_ds = TransactionDataset(\n",
    "    df[lambda x: x[\"client_id\"].isin(clients_train)], \n",
    "    id_col=\"client_id\", \n",
    "    dt_col=\"TRDATETIME\", \n",
    "    cat_cols=[\"small_group\", \"amount_rur_bin\"],\n",
    "    min_length=20,\n",
    "    max_length=100\n",
    ")\n",
    "\n",
    "val_ds = TransactionDataset(\n",
    "    df[lambda x: x[\"client_id\"].isin(clients_val)], \n",
    "    id_col=\"client_id\", \n",
    "    dt_col=\"TRDATETIME\", \n",
    "    cat_cols=[\"small_group\", \"amount_rur_bin\"],\n",
    "    min_length=20,\n",
    "    max_length=100\n",
    ")\n",
    "\n",
    "test_ds = TransactionDataset(\n",
    "    df[lambda x: x[\"client_id\"].isin(clients_test)], \n",
    "    id_col=\"client_id\", \n",
    "    dt_col=\"TRDATETIME\", \n",
    "    cat_cols=[\"small_group\", \"amount_rur_bin\"],\n",
    "    min_length=20,\n",
    "    max_length=100\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install performer-pytorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from performer_pytorch import Performer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "unable to import cuda code for auto-regressive Performer. will default to the memory inefficient non-cuda version\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 2048, 512])"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "from performer_pytorch import Performer\n",
    "\n",
    "model = Performer(\n",
    "    dim = 512,\n",
    "    depth = 1,\n",
    "    heads = 8,\n",
    "    causal = True,\n",
    "    dim_head = 64\n",
    ")\n",
    "\n",
    "x = torch.randn(1, 2048, 512)\n",
    "model(x).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "class TransactionEncoder(nn.Module):\n",
    "    def __init__(self, feature_embeddings: dict[str, tuple[int, int]], linear_proj: int=None):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.feature_embeddings = feature_embeddings\n",
    "        self.embeddings = nn.ModuleDict({key: nn.Embedding(vocab, dim) for key, (vocab, dim) in feature_embeddings.items()})\n",
    "        \n",
    "        if linear_proj is not None:\n",
    "            self.embedding_dim = linear_proj\n",
    "            self.linear_proj = nn.Linear(sum([dim for key, (vocab, dim) in feature_embeddings.items()]), linear_proj)\n",
    "        else:\n",
    "            self.embedding_dim = sum([dim for key, (vocab, dim) in feature_embeddings.items()])\n",
    "            self.linear_proj = nn.Identity()\n",
    "\n",
    "    def forward(self, x: torch.Tensor, device: str=\"cpu\") -> torch.Tensor:\n",
    "        embeddings = [self.embeddings[key](x[key].to(device)) for key in self.feature_embeddings]\n",
    "        proj = self.linear_proj(torch.cat(embeddings, dim=2))\n",
    "        return proj\n",
    "\n",
    "\n",
    "class Head(nn.Module):\n",
    "    def __init__(self, embedding_dim: int, hidden_dim: int, vocab_size: int):\n",
    "        super().__init__()\n",
    "\n",
    "        self.head = nn.Sequential(\n",
    "            nn.Linear(embedding_dim, hidden_dim),\n",
    "            nn.GELU(),\n",
    "            nn.Linear(hidden_dim, vocab_size),\n",
    "        )\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        return self.head(x)\n",
    "\n",
    "\n",
    "class PositionalEncoding(nn.Module):\n",
    "    def __init__(self, embedding_dim: int, dropout: float=0.1, max_len: int=1000):\n",
    "        super().__init__()\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "        position = torch.arange(max_len).unsqueeze(1)\n",
    "        div_term = torch.exp(torch.arange(0, embedding_dim, 2) * (-math.log(10000.0) / embedding_dim))\n",
    "        pe = torch.zeros(1, max_len, embedding_dim)\n",
    "        pe[0, :, 0::2] = torch.sin(position * div_term)\n",
    "        pe[0, :, 1::2] = torch.cos(position * div_term)\n",
    "        self.register_buffer('pe', pe)\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Arguments:\n",
    "            x: Tensor, shape ``[batch_size, seq_len, embedding_dim]``\n",
    "        \"\"\"\n",
    "        x = x + self.pe[:, :x.size(1), :]\n",
    "        return self.dropout(x)\n",
    "\n",
    "\n",
    "class PerformerModel(nn.Module):\n",
    "    def __init__(\n",
    "            self, \n",
    "            feature_embeddings: dict[str, tuple[int, int]], \n",
    "            linear_proj: int=None,\n",
    "            n_head: int=8, \n",
    "            dim_feedforward: int=128, \n",
    "            dropout: float=0.1, \n",
    "            num_layers: int=6, \n",
    "            head_hidden: int=128,\n",
    "            max_len: int=1000,\n",
    "        ):\n",
    "        super().__init__()\n",
    "\n",
    "        self.transaction_encoder = TransactionEncoder(feature_embeddings, linear_proj=linear_proj)\n",
    "        self.embedding_dim = self.transaction_encoder.embedding_dim\n",
    "        self.cat_cols = list(feature_embeddings.keys())\n",
    "        self.num_classes_dict = {key: num_classes for key, (num_classes, _) in feature_embeddings.items()}\n",
    "        \n",
    "        self.pos_emb = PositionalEncoding(self.embedding_dim, dropout, max_len)\n",
    "\n",
    "        self.transformer_encoder = Performer(\n",
    "            dim = self.embedding_dim, \n",
    "            depth = num_layers,\n",
    "            heads = n_head, \n",
    "            ff_dropout = dropout,\n",
    "            causal = True,\n",
    "            dim_head = 16,\n",
    "            # use_rezero = True,\n",
    "            # no_projection = True,\n",
    "            # feature_redraw_interval = 10000000,\n",
    "            # bucket_size = 25\n",
    "        )\n",
    "        \n",
    "        self.heads = nn.ModuleDict({\n",
    "            key: Head(\n",
    "                self.embedding_dim, \n",
    "                head_hidden, \n",
    "                num_classes\n",
    "            ) for key, num_classes in self.num_classes_dict.items()\n",
    "        })\n",
    "\n",
    "    def forward(self, x: torch.Tensor, device: str=\"cpu\") -> torch.Tensor:\n",
    "        N, S = x[self.cat_cols[0]].shape\n",
    "        embeddings = self.transaction_encoder(x, device=device)\n",
    "        embeddings = self.pos_emb(embeddings)\n",
    "        \n",
    "        attn_mask = self.generate_square_subsequent_mask(S).to(device)\n",
    "        padding_mask = self.generate_padding_mask(x[self.cat_cols[0]]).to(device)\n",
    "        embeddings = self.transformer_encoder(embeddings, input_mask=padding_mask)\n",
    "\n",
    "        logits = {key: self.heads[key](embeddings) for key in self.cat_cols}\n",
    "        return logits\n",
    "\n",
    "    @staticmethod\n",
    "    def generate_square_subsequent_mask(sz: int) -> torch.Tensor:\n",
    "        return torch.triu(torch.full((sz, sz), True), diagonal=1).bool()\n",
    "    \n",
    "    @staticmethod\n",
    "    def generate_padding_mask(x: torch.Tensor) -> torch.Tensor:\n",
    "        return torch.where(x == 0, True, False).bool()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm.notebook import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "def train_epoch(model, optimizer, dataloader, warmup=10, device=\"cuda\"):\n",
    "    model.train()\n",
    "    model.to(device)\n",
    "\n",
    "    metrics = {\n",
    "        key: {\n",
    "            \"f1_score\": MulticlassF1Score(\n",
    "                num_classes=num_classes, \n",
    "                average=\"weighted\", \n",
    "                ignore_index=0\n",
    "            ), \n",
    "            \"accuracy\": Accuracy(\n",
    "                task=\"multiclass\", \n",
    "                num_classes=num_classes, \n",
    "                ignore_index=0\n",
    "            )\n",
    "        } for key, num_classes in model.num_classes_dict.items()\n",
    "    }\n",
    "\n",
    "    loss_epoch = 0\n",
    "    count = 0 \n",
    "    for batch_dict in tqdm(dataloader):\n",
    "        logits_dict = model(batch_dict, device=device)\n",
    "\n",
    "        loss = 0\n",
    "        for key, logits in logits_dict.items():\n",
    "            y = batch_dict[key][:, warmup + 1:].to(device)\n",
    "            logits_pred = logits[:, warmup: -1].permute(0, 2, 1)    # B x C x T\n",
    "\n",
    "            loss += nn.functional.cross_entropy(logits_pred, y, ignore_index=0)\n",
    "            \n",
    "            y_pred = logits_pred.argmax(dim=1).to(\"cpu\")\n",
    "            metrics[key][\"f1_score\"].update(y_pred, y.to(\"cpu\"))\n",
    "            metrics[key][\"accuracy\"].update(y_pred, y.to(\"cpu\"))\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        cur_count =  torch.sum((y != 0).float()).item()\n",
    "        loss_epoch += loss.item() * cur_count\n",
    "        count += cur_count\n",
    "\n",
    "    return loss_epoch / count, {feature: {m: v.compute().item() for m, v in results.items()} for feature, results in metrics.items()}\n",
    "\n",
    "\n",
    "def eval_epoch(model, dataloader, warmup=10, device=\"cuda\"):\n",
    "    model.eval()\n",
    "    model.to(device)\n",
    "\n",
    "    metrics = {\n",
    "        key: {\n",
    "            \"f1_score\": MulticlassF1Score(\n",
    "                num_classes=num_classes, \n",
    "                average=\"weighted\", \n",
    "                ignore_index=0\n",
    "            ), \n",
    "            \"accuracy\": Accuracy(\n",
    "                task=\"multiclass\", \n",
    "                num_classes=num_classes, \n",
    "                ignore_index=0\n",
    "            )\n",
    "        } for key, num_classes in model.num_classes_dict.items()\n",
    "    }\n",
    "\n",
    "    with torch.no_grad():\n",
    "        loss_epoch = 0\n",
    "        count = 0 \n",
    "        for batch_dict in dataloader:\n",
    "            logits_dict = model(batch_dict, device=device)\n",
    "\n",
    "            loss = 0\n",
    "            for key, logits in logits_dict.items():\n",
    "                y = batch_dict[key][:, warmup + 1:].to(device)\n",
    "                logits_pred = logits[:, warmup: -1].permute(0, 2, 1)\n",
    "        \n",
    "                loss += nn.functional.cross_entropy(logits_pred, y, ignore_index=0)\n",
    "\n",
    "                y_pred = logits_pred.argmax(dim=1).to(\"cpu\")\n",
    "                metrics[key][\"f1_score\"].update(y_pred, y.to(\"cpu\"))\n",
    "                metrics[key][\"accuracy\"].update(y_pred, y.to(\"cpu\"))\n",
    "\n",
    "            cur_count = torch.sum((y != 0).float()).item()\n",
    "            loss_epoch += loss.item() * cur_count\n",
    "            count += cur_count    \n",
    "\n",
    "    return loss_epoch / count, {feature: {m: v.compute().item() for m, v in results.items()} for feature, results in metrics.items()}\n",
    "\n",
    "\n",
    "def train_model(model, optimizer, dataloaders, n_epochs, warmup=10, device=\"cuda\", save_path=\"./\"):\n",
    "    if not os.path.exists(save_path):\n",
    "        os.mkdir(save_path)\n",
    "\n",
    "    best_loss = float(\"inf\")\n",
    "    for epoch in tqdm(range(n_epochs)):\n",
    "        train_start = time.perf_counter()\n",
    "        train_loss, train_metrics = train_epoch(model, optimizer, dataloaders[\"train\"], warmup, device)\n",
    "        train_end = time.perf_counter()\n",
    "        val_loss, val_metrics = eval_epoch(model, dataloaders[\"val\"], warmup, device)\n",
    "        val_end = time.perf_counter()\n",
    "\n",
    "        # wandb.log({\n",
    "        #     \"Epoch\": epoch+1,\n",
    "        #     \"Train time\": train_end - train_start,\n",
    "        #     \"Train loss\": train_loss,\n",
    "        #     \"Train metrics\": train_metrics,\n",
    "        #     \"Val time\": val_end - train_end,\n",
    "        #     \"Val metrics\": val_metrics,\n",
    "        #     \"Val loss\": val_loss\n",
    "        # })\n",
    "\n",
    "        if val_loss < best_loss:\n",
    "             best_loss = val_loss\n",
    "             torch.save(model, os.path.join(save_path, \"best_model.pt\"))\n",
    "\n",
    "    model = torch.load(os.path.join(save_path, \"best_model.pt\"))\n",
    "\n",
    "    test_start = time.perf_counter()\n",
    "    test_loss, test_metrics = eval_epoch(model, dataloaders[\"test\"], warmup, device)\n",
    "    test_end = time.perf_counter()\n",
    "\n",
    "    # wandb.summary[\"Test time\"] = test_end - test_start\n",
    "    # wandb.summary[\"Test metrics\"] = test_metrics\n",
    "    # wandb.summary[\"Test loss\"] = test_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = DataLoader(train_ds, batch_size=32, shuffle=True, collate_fn=transaction_collate_fn)\n",
    "val_loader = DataLoader(val_ds, batch_size=32, shuffle=False, collate_fn=transaction_collate_fn)\n",
    "test_loader = DataLoader(test_ds, batch_size=32, shuffle=False, collate_fn=transaction_collate_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "unable to import cuda code for auto-regressive Performer. will default to the memory inefficient non-cuda version\n",
      "unable to import cuda code for auto-regressive Performer. will default to the memory inefficient non-cuda version\n",
      "unable to import cuda code for auto-regressive Performer. will default to the memory inefficient non-cuda version\n",
      "unable to import cuda code for auto-regressive Performer. will default to the memory inefficient non-cuda version\n",
      "unable to import cuda code for auto-regressive Performer. will default to the memory inefficient non-cuda version\n",
      "unable to import cuda code for auto-regressive Performer. will default to the memory inefficient non-cuda version\n"
     ]
    }
   ],
   "source": [
    "transformer = PerformerModel(\n",
    "    feature_embeddings={\"small_group\": (345, 64), \"amount_rur_bin\": (11, 64)}, \n",
    "    linear_proj=64,\n",
    "    n_head=8, \n",
    "    dim_feedforward=128, \n",
    "    dropout=0.1, \n",
    "    num_layers=6, \n",
    "    head_hidden=128\n",
    ")\n",
    "\n",
    "optimizer = torch.optim.Adam(transformer.parameters(), lr=1e-3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e34d895e0c0a46c78fedea987d1824ed",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/20 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "64d15ff93eeb44f199a5396c8f2f2916",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/106 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[86], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m train_model(\n\u001b[1;32m      2\u001b[0m     transformer, \n\u001b[1;32m      3\u001b[0m     optimizer,\n\u001b[1;32m      4\u001b[0m     {\u001b[39m\"\u001b[39;49m\u001b[39mtrain\u001b[39;49m\u001b[39m\"\u001b[39;49m: train_loader, \u001b[39m\"\u001b[39;49m\u001b[39mval\u001b[39;49m\u001b[39m\"\u001b[39;49m: val_loader, \u001b[39m\"\u001b[39;49m\u001b[39mtest\u001b[39;49m\u001b[39m\"\u001b[39;49m: test_loader},\n\u001b[1;32m      5\u001b[0m     \u001b[39m20\u001b[39;49m\n\u001b[1;32m      6\u001b[0m )\n",
      "Cell \u001b[0;32mIn[83], line 99\u001b[0m, in \u001b[0;36mtrain_model\u001b[0;34m(model, optimizer, dataloaders, n_epochs, warmup, device, save_path)\u001b[0m\n\u001b[1;32m     97\u001b[0m \u001b[39mfor\u001b[39;00m epoch \u001b[39min\u001b[39;00m tqdm(\u001b[39mrange\u001b[39m(n_epochs)):\n\u001b[1;32m     98\u001b[0m     train_start \u001b[39m=\u001b[39m time\u001b[39m.\u001b[39mperf_counter()\n\u001b[0;32m---> 99\u001b[0m     train_loss, train_metrics \u001b[39m=\u001b[39m train_epoch(model, optimizer, dataloaders[\u001b[39m\"\u001b[39;49m\u001b[39mtrain\u001b[39;49m\u001b[39m\"\u001b[39;49m], warmup, device)\n\u001b[1;32m    100\u001b[0m     train_end \u001b[39m=\u001b[39m time\u001b[39m.\u001b[39mperf_counter()\n\u001b[1;32m    101\u001b[0m     val_loss, val_metrics \u001b[39m=\u001b[39m eval_epoch(model, dataloaders[\u001b[39m\"\u001b[39m\u001b[39mval\u001b[39m\u001b[39m\"\u001b[39m], warmup, device)\n",
      "Cell \u001b[0;32mIn[83], line 25\u001b[0m, in \u001b[0;36mtrain_epoch\u001b[0;34m(model, optimizer, dataloader, warmup, device)\u001b[0m\n\u001b[1;32m     23\u001b[0m count \u001b[39m=\u001b[39m \u001b[39m0\u001b[39m \n\u001b[1;32m     24\u001b[0m \u001b[39mfor\u001b[39;00m batch_dict \u001b[39min\u001b[39;00m tqdm(dataloader):\n\u001b[0;32m---> 25\u001b[0m     logits_dict \u001b[39m=\u001b[39m model(batch_dict, device\u001b[39m=\u001b[39;49mdevice)\n\u001b[1;32m     27\u001b[0m     loss \u001b[39m=\u001b[39m \u001b[39m0\u001b[39m\n\u001b[1;32m     28\u001b[0m     \u001b[39mfor\u001b[39;00m key, logits \u001b[39min\u001b[39;00m logits_dict\u001b[39m.\u001b[39mitems():\n",
      "File \u001b[0;32m~/.pyenv/versions/3.9.16/envs/ML_project/lib/python3.9/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "Cell \u001b[0;32mIn[81], line 106\u001b[0m, in \u001b[0;36mPerformerModel.forward\u001b[0;34m(self, x, device)\u001b[0m\n\u001b[1;32m    104\u001b[0m attn_mask \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mgenerate_square_subsequent_mask(S)\u001b[39m.\u001b[39mto(device)\n\u001b[1;32m    105\u001b[0m padding_mask \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mgenerate_padding_mask(x[\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcat_cols[\u001b[39m0\u001b[39m]])\u001b[39m.\u001b[39mto(device)\n\u001b[0;32m--> 106\u001b[0m embeddings \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtransformer_encoder(embeddings, input_mask\u001b[39m=\u001b[39;49mpadding_mask)\n\u001b[1;32m    108\u001b[0m logits \u001b[39m=\u001b[39m {key: \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mheads[key](embeddings) \u001b[39mfor\u001b[39;00m key \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcat_cols}\n\u001b[1;32m    109\u001b[0m \u001b[39mreturn\u001b[39;00m logits\n",
      "File \u001b[0;32m~/.pyenv/versions/3.9.16/envs/ML_project/lib/python3.9/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/.pyenv/versions/3.9.16/envs/ML_project/lib/python3.9/site-packages/performer_pytorch/performer_pytorch.py:579\u001b[0m, in \u001b[0;36mPerformer.forward\u001b[0;34m(self, x, **kwargs)\u001b[0m\n\u001b[1;32m    577\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mauto_check_redraw:\n\u001b[1;32m    578\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mproj_updater\u001b[39m.\u001b[39mredraw_projections()\n\u001b[0;32m--> 579\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mnet(x, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/.pyenv/versions/3.9.16/envs/ML_project/lib/python3.9/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/.pyenv/versions/3.9.16/envs/ML_project/lib/python3.9/site-packages/performer_pytorch/reversible.py:138\u001b[0m, in \u001b[0;36mSequentialSequence.forward\u001b[0;34m(self, x, **kwargs)\u001b[0m\n\u001b[1;32m    135\u001b[0m layers_and_args \u001b[39m=\u001b[39m \u001b[39mlist\u001b[39m(\u001b[39mzip\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlayers, args))\n\u001b[1;32m    137\u001b[0m \u001b[39mfor\u001b[39;00m (f, g), (f_args, g_args) \u001b[39min\u001b[39;00m layers_and_args:\n\u001b[0;32m--> 138\u001b[0m     x \u001b[39m=\u001b[39m x \u001b[39m+\u001b[39m f(x, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mf_args)\n\u001b[1;32m    139\u001b[0m     x \u001b[39m=\u001b[39m x \u001b[39m+\u001b[39m g(x, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mg_args)\n\u001b[1;32m    140\u001b[0m \u001b[39mreturn\u001b[39;00m x\n",
      "File \u001b[0;32m~/.pyenv/versions/3.9.16/envs/ML_project/lib/python3.9/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/.pyenv/versions/3.9.16/envs/ML_project/lib/python3.9/site-packages/performer_pytorch/performer_pytorch.py:338\u001b[0m, in \u001b[0;36mPreLayerNorm.forward\u001b[0;34m(self, x, **kwargs)\u001b[0m\n\u001b[1;32m    337\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, x, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[0;32m--> 338\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mfn(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mnorm(x), \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/.pyenv/versions/3.9.16/envs/ML_project/lib/python3.9/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/.pyenv/versions/3.9.16/envs/ML_project/lib/python3.9/site-packages/performer_pytorch/performer_pytorch.py:449\u001b[0m, in \u001b[0;36mSelfAttention.forward\u001b[0;34m(self, context, *args, **kwargs)\u001b[0m\n\u001b[1;32m    447\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39m*\u001b[39margs, context \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[1;32m    448\u001b[0m     \u001b[39massert\u001b[39;00m \u001b[39mnot\u001b[39;00m exists(context), \u001b[39m'\u001b[39m\u001b[39mself attention should not receive context\u001b[39m\u001b[39m'\u001b[39m\n\u001b[0;32m--> 449\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39msuper\u001b[39;49m()\u001b[39m.\u001b[39;49mforward(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/.pyenv/versions/3.9.16/envs/ML_project/lib/python3.9/site-packages/performer_pytorch/performer_pytorch.py:433\u001b[0m, in \u001b[0;36mAttention.forward\u001b[0;34m(self, x, pos_emb, context, mask, context_mask, **kwargs)\u001b[0m\n\u001b[1;32m    430\u001b[0m     \u001b[39mif\u001b[39;00m exists(pos_emb) \u001b[39mand\u001b[39;00m \u001b[39mnot\u001b[39;00m cross_attend:\n\u001b[1;32m    431\u001b[0m         q, k \u001b[39m=\u001b[39m apply_rotary_pos_emb(q, k, pos_emb)\n\u001b[0;32m--> 433\u001b[0m     out \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mfast_attention(q, k, v)\n\u001b[1;32m    434\u001b[0m     attn_outs\u001b[39m.\u001b[39mappend(out)\n\u001b[1;32m    436\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m empty(lq):\n",
      "File \u001b[0;32m~/.pyenv/versions/3.9.16/envs/ML_project/lib/python3.9/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/.pyenv/versions/3.9.16/envs/ML_project/lib/python3.9/site-packages/performer_pytorch/performer_pytorch.py:273\u001b[0m, in \u001b[0;36mFastAttention.forward\u001b[0;34m(self, q, k, v)\u001b[0m\n\u001b[1;32m    270\u001b[0m     k \u001b[39m=\u001b[39m create_kernel(k, is_query \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m)\n\u001b[1;32m    272\u001b[0m attn_fn \u001b[39m=\u001b[39m linear_attention \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcausal \u001b[39melse\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcausal_linear_fn\n\u001b[0;32m--> 273\u001b[0m out \u001b[39m=\u001b[39m attn_fn(q, k, v)\n\u001b[1;32m    274\u001b[0m \u001b[39mreturn\u001b[39;00m out\n",
      "File \u001b[0;32m~/.pyenv/versions/3.9.16/envs/ML_project/lib/python3.9/site-packages/performer_pytorch/performer_pytorch.py:212\u001b[0m, in \u001b[0;36mcausal_linear_attention_noncuda\u001b[0;34m(q, k, v, chunk_size, eps)\u001b[0m\n\u001b[1;32m    210\u001b[0m D_inv \u001b[39m=\u001b[39m \u001b[39m1.\u001b[39m \u001b[39m/\u001b[39m torch\u001b[39m.\u001b[39meinsum(\u001b[39m'\u001b[39m\u001b[39m...nd,...nd->...n\u001b[39m\u001b[39m'\u001b[39m, q, k_cumsum\u001b[39m.\u001b[39mtype_as(q) \u001b[39m+\u001b[39m eps)\n\u001b[1;32m    211\u001b[0m context \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39meinsum(\u001b[39m'\u001b[39m\u001b[39m...nd,...ne->...nde\u001b[39m\u001b[39m'\u001b[39m, k, v)\n\u001b[0;32m--> 212\u001b[0m context_cumsum \u001b[39m=\u001b[39m last_context_cumsum \u001b[39m+\u001b[39m context\u001b[39m.\u001b[39;49mcumsum(dim\u001b[39m=\u001b[39;49m\u001b[39m-\u001b[39;49m\u001b[39m3\u001b[39;49m)\n\u001b[1;32m    213\u001b[0m out \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39meinsum(\u001b[39m'\u001b[39m\u001b[39m...nde,...nd,...n->...ne\u001b[39m\u001b[39m'\u001b[39m, context_cumsum, q, D_inv)\n\u001b[1;32m    215\u001b[0m last_k_cumsum \u001b[39m=\u001b[39m k_cumsum[:, :, \u001b[39m-\u001b[39m\u001b[39m1\u001b[39m:]\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "train_model(\n",
    "    transformer, \n",
    "    optimizer,\n",
    "    {\"train\": train_loader, \"val\": val_loader, \"test\": test_loader},\n",
    "    20\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = df.sort_values(\"TRDATETIME\").groupby(\"client_id\").apply(lambda x: x[\"amount_rur_bin\"] == x[\"amount_rur_bin\"].shift())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.17164274952957415"
      ]
     },
     "execution_count": 111,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a.reset_index()[\"amount_rur_bin\"].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = {\n",
    "    \"experiment_name\": \"run2\",\n",
    "    \"dataset\": \"rosbank\",\n",
    "    \"min_length\": 20,\n",
    "    \"max_length\": 100,\n",
    "    \"batch_size\": 32,\n",
    "    \"type\": \"transformer\",\n",
    "    \"transformer_params\": {\n",
    "        \"feature_embeddings\": {\"small_group\": (345, 64), \"amount_rur_bin\": (11, 64)}, \n",
    "        \"linear_proj\": 64,\n",
    "        \"n_head\": 8, \n",
    "        \"dim_feedforward\": 128, \n",
    "        \"dropout\": 0.1, \n",
    "        \"num_layers\": 6, \n",
    "        \"head_hidden\": 128,\n",
    "    },\n",
    "    \"lr\": 1e-3,\n",
    "    \"n_epochs\": 50,\n",
    "    \"warmup\": 10,\n",
    "    \"device\": \"cuda\",\n",
    "    \"save_path\": \"./\"\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "with open(\"config.json\", \"w\") as f:\n",
    "    json.dump(config, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "from yaml import load, dump\n",
    "# from yaml import CLoader as Loader, CDumper as Dumper\n",
    "\n",
    "with open(\"config.yaml\", \"w\") as f:\n",
    "    dump(config, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'././'"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.path.exists(os.path.join(\"./\", \"./\"))\n",
    "os.path.join(\"./\", \"./\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "load() missing 1 required positional argument: 'Loader'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[59], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[39mwith\u001b[39;00m \u001b[39mopen\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39mconfig.yaml\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mr\u001b[39m\u001b[39m\"\u001b[39m) \u001b[39mas\u001b[39;00m f:\n\u001b[1;32m----> 2\u001b[0m     d \u001b[39m=\u001b[39m load(f)\n\u001b[0;32m      4\u001b[0m d\n",
      "\u001b[1;31mTypeError\u001b[0m: load() missing 1 required positional argument: 'Loader'"
     ]
    }
   ],
   "source": [
    "\n",
    "with open(\"config.yaml\", \"r\") as f:\n",
    "    d = load(f)\n",
    "\n",
    "d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main(config):\n",
    "    # wandb.login()\n",
    "\n",
    "    # wandb.init(\n",
    "    #     project=\"deep-learning-project\",\n",
    "    #     name=config[\"experiment_name\"], \n",
    "    #     tags=config[\"tags\"],\n",
    "    #     config=config\n",
    "    # )\n",
    "\n",
    "    if config[\"dataset\"] == \"rosbank\":\n",
    "        df = pd.read_csv('data/rosbank/train.csv')\n",
    "        df['TRDATETIME'] = pd.to_datetime(df['TRDATETIME'], format='%d%b%y:%H:%M:%S')\n",
    "        df = df.rename(columns={'cl_id':'client_id', 'MCC':'small_group', 'amount':'amount_rur'})\n",
    "        \n",
    "        mcc_to_id = {mcc: i+1 for i, mcc in enumerate(df['small_group'].unique())}\n",
    "\n",
    "        df['amount_rur_bin'] = 1 + KBinsDiscretizer(10, encode='ordinal', subsample=None).fit_transform(df[['amount_rur']]).astype('int')\n",
    "        df['small_group'] = df['small_group'].map(mcc_to_id)\n",
    "\n",
    "    else:\n",
    "        pass\n",
    "\n",
    "    clients_train, clients_val_test = train_test_split(df[\"client_id\"].unique(), test_size=0.2, random_state=42)\n",
    "    clients_val, clients_test = train_test_split(clients_val_test, test_size=0.5, random_state=42)\n",
    "\n",
    "    train_ds = TransactionDataset(\n",
    "        df[lambda x: x[\"client_id\"].isin(clients_train)], \n",
    "        id_col=\"client_id\", \n",
    "        dt_col=\"TRDATETIME\", \n",
    "        cat_cols=[\"small_group\", \"amount_rur_bin\"],\n",
    "        min_length=config[\"min_length\"],\n",
    "        max_length=config[\"max_length\"],\n",
    "        random_slice=True\n",
    "    )\n",
    "\n",
    "    val_ds = TransactionDataset(\n",
    "        df[lambda x: x[\"client_id\"].isin(clients_val)], \n",
    "        id_col=\"client_id\", \n",
    "        dt_col=\"TRDATETIME\", \n",
    "        cat_cols=[\"small_group\", \"amount_rur_bin\"],\n",
    "        min_length=config[\"min_length\"],\n",
    "        max_length=config[\"max_length\"],\n",
    "        random_slice=False\n",
    "    )\n",
    "\n",
    "    test_ds = TransactionDataset(\n",
    "        df[lambda x: x[\"client_id\"].isin(clients_test)], \n",
    "        id_col=\"client_id\", \n",
    "        dt_col=\"TRDATETIME\", \n",
    "        cat_cols=[\"small_group\", \"amount_rur_bin\"],\n",
    "        min_length=config[\"min_length\"],\n",
    "        max_length=config[\"max_length\"],\n",
    "        random_slice=False\n",
    "    )\n",
    "\n",
    "    train_loader = DataLoader(train_ds, batch_size=config[\"batch_size\"], shuffle=True, collate_fn=transaction_collate_fn)\n",
    "    val_loader = DataLoader(val_ds, batch_size=config[\"batch_size\"], shuffle=False, collate_fn=transaction_collate_fn)\n",
    "    test_loader = DataLoader(test_ds, batch_size=config[\"batch_size\"], shuffle=False, collate_fn=transaction_collate_fn)\n",
    "\n",
    "    transformer = TransformerModel(**config[\"transformer_params\"])\n",
    "    optimizer = torch.optim.Adam(transformer.parameters(), lr=config[\"lr\"])\n",
    "\n",
    "    train_model(\n",
    "        transformer, \n",
    "        optimizer, \n",
    "        {\"train\": train_loader, \"val\": val_loader, \"test\": test_loader}, \n",
    "        n_epochs=config[\"n_epochs\"],\n",
    "        warmup=config[\"warmup\"],\n",
    "        device=config[\"device\"],\n",
    "        save_path=config[\"save_path\"]\n",
    "    )\n",
    "\n",
    "    # wandb.finish()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Calling wandb.login() after wandb.init() has no effect.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Finishing last run (ID:bu3ggt4d) before initializing another..."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a3d4f01021c4472eafce6656a6a5a0b8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.001 MB of 0.001 MB uploaded (0.000 MB deduped)\\r'), FloatProgress(value=1.0, max…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">run2</strong> at: <a href='https://wandb.ai/aayugay99/deep-learning-project/runs/bu3ggt4d' target=\"_blank\">https://wandb.ai/aayugay99/deep-learning-project/runs/bu3ggt4d</a><br/>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>.\\wandb\\run-20230522_184446-bu3ggt4d\\logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Successfully finished last run (ID:bu3ggt4d). Initializing new run:<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "69ac9dc73049497f8e49d12a39acf682",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='Waiting for wandb.init()...\\r'), FloatProgress(value=0.016666666666666666, max=1.0…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.15.3 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.14.0"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>d:\\Skoltech\\Deep Learning\\dl2023\\efficient_transformers_on_transactional_data\\wandb\\run-20230522_184723-cgbc8t2b</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/aayugay99/deep-learning-project/runs/cgbc8t2b' target=\"_blank\">run2</a></strong> to <a href='https://wandb.ai/aayugay99/deep-learning-project' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/aayugay99/deep-learning-project' target=\"_blank\">https://wandb.ai/aayugay99/deep-learning-project</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/aayugay99/deep-learning-project/runs/cgbc8t2b' target=\"_blank\">https://wandb.ai/aayugay99/deep-learning-project/runs/cgbc8t2b</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "628 sequences were filtered\n",
      "77 sequences were filtered\n",
      "70 sequences were filtered\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\User\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\torch\\nn\\modules\\transformer.py:562: UserWarning: Converting mask without torch.bool dtype to bool; this will negatively affect performance. Prefer to use a boolean mask directly. (Triggered internally at C:\\actions-runner\\_work\\pytorch\\pytorch\\builder\\windows\\pytorch\\aten\\src\\ATen\\native\\transformers\\attention.cpp:152.)\n",
      "  return torch._transformer_encoder_layer_fwd(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>Epoch</td><td>▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇███</td></tr><tr><td>Train loss</td><td>█▅▄▃▃▃▃▃▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>Train time</td><td>█▅▂▁▁▁▂▁▁▁▁▁▂▁▁▂▁▂▁▁▁▁▂▁▂▁▁▁▁▂▂▁▂▂▂▂▁▁▁▁</td></tr><tr><td>Val loss</td><td>█▅▄▄▃▂▂▂▂▁▁▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▂▂▂▁▂</td></tr><tr><td>Val time</td><td>▂▃▁▃▂▁▁▁▁▁▃▃▂▁▁▂▁▁▁▁▁▁▁█▁▁▂▁▂█▂▃▂▃▂▆▁▁▃▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>Epoch</td><td>50</td></tr><tr><td>Test loss</td><td>4.94315</td></tr><tr><td>Test time</td><td>0.19568</td></tr><tr><td>Train loss</td><td>4.75581</td></tr><tr><td>Train time</td><td>3.7141</td></tr><tr><td>Val loss</td><td>5.037</td></tr><tr><td>Val time</td><td>0.19415</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">run2</strong> at: <a href='https://wandb.ai/aayugay99/deep-learning-project/runs/cgbc8t2b' target=\"_blank\">https://wandb.ai/aayugay99/deep-learning-project/runs/cgbc8t2b</a><br/>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>.\\wandb\\run-20230522_184723-cgbc8t2b\\logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "main(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wandb.login()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "cf92aa13fedf815d5c8dd192b8d835913fde3e8bc926b2a0ad6cc74ef2ba3ca2"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
